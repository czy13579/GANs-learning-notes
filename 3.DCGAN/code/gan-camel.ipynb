{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":29849,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!python --version","metadata":{"execution":{"iopub.status.busy":"2024-01-20T05:16:26.077294Z","iopub.execute_input":"2024-01-20T05:16:26.078017Z","iopub.status.idle":"2024-01-20T05:16:27.205995Z","shell.execute_reply.started":"2024-01-20T05:16:26.077934Z","shell.execute_reply":"2024-01-20T05:16:27.204785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/davidADSP/GDL_code","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:26:01.448154Z","iopub.execute_input":"2024-03-19T03:26:01.448665Z","iopub.status.idle":"2024-03-19T03:26:03.587777Z","shell.execute_reply.started":"2024-03-19T03:26:01.448612Z","shell.execute_reply":"2024-03-19T03:26:03.586543Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'GDL_code'...\nremote: Enumerating objects: 394, done.\u001b[K\nremote: Counting objects: 100% (3/3), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 394 (delta 0), reused 1 (delta 0), pack-reused 391\u001b[K\nReceiving objects: 100% (394/394), 22.13 MiB | 0 bytes/s, done.\nResolving deltas: 100% (237/237), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"cd /kaggle/working/GDL_code","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:26:47.980076Z","iopub.execute_input":"2024-03-19T03:26:47.980469Z","iopub.status.idle":"2024-03-19T03:26:47.988471Z","shell.execute_reply.started":"2024-03-19T03:26:47.980412Z","shell.execute_reply":"2024-03-19T03:26:47.987553Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/GDL_code\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install gsutil ","metadata":{"execution":{"iopub.status.busy":"2024-03-19T04:02:37.899814Z","iopub.execute_input":"2024-03-19T04:02:37.900247Z","iopub.status.idle":"2024-03-19T04:03:26.441979Z","shell.execute_reply.started":"2024-03-19T04:02:37.900192Z","shell.execute_reply":"2024-03-19T04:03:26.440971Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Collecting gsutil\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/ce/9c70a91e1a5fc709e6acf34682b8b2499179ddc27b18b0e3670ff9c257db/gsutil-5.27.tar.gz (3.0MB)\n\u001b[K     |████████████████████████████████| 3.0MB 3.8MB/s eta 0:00:01\n\u001b[?25hCollecting argcomplete>=1.9.4\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/05/223116a4a5905d6b26bff334ffc7b74474fafe23fcb10532caf0ef86ca69/argcomplete-3.1.2-py3-none-any.whl (41kB)\n\u001b[K     |████████████████████████████████| 51kB 6.0MB/s  eta 0:00:01\n\u001b[?25hCollecting crcmod>=1.7\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/b0/e595ce2a2527e169c3bcd6c33d2473c1918e0b7f6826a043ca1245dd4e5b/crcmod-1.7.tar.gz (89kB)\n\u001b[K     |████████████████████████████████| 92kB 9.3MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: fasteners>=0.14.1 in /opt/conda/lib/python3.6/site-packages (from gsutil) (0.14.1)\nCollecting gcs-oauth2-boto-plugin>=3.0\n  Downloading https://files.pythonhosted.org/packages/05/e5/3162be0abab32f152f331423426471935f286dd4ad70fa704f2a34ea3c1e/gcs-oauth2-boto-plugin-3.0.tar.gz\nCollecting google-apitools>=0.5.32\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/cb/cb0311f2ec371c83d6510847476c665edc9cc97564a51923557bc8f0b680/google_apitools-0.5.32-py3-none-any.whl (135kB)\n\u001b[K     |████████████████████████████████| 143kB 32.7MB/s eta 0:00:01\n\u001b[?25hCollecting httplib2==0.20.4\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/0f/29725a9caf4b2618f524e0f28e2bda91aca8f880123ec77426ede6ea1ea4/httplib2-0.20.4-py3-none-any.whl (96kB)\n\u001b[K     |████████████████████████████████| 102kB 10.9MB/s ta 0:00:01\n\u001b[?25hCollecting google-reauth>=0.1.0\n  Downloading https://files.pythonhosted.org/packages/69/e1/67ffaa3a645b86318ce30717af7145070ebccec5eef5c623ae08b86129b8/google_reauth-0.1.1-py2.py3-none-any.whl\nRequirement already satisfied: monotonic>=1.4 in /opt/conda/lib/python3.6/site-packages (from gsutil) (1.5)\nRequirement already satisfied: pyOpenSSL>=0.13 in /opt/conda/lib/python3.6/site-packages (from gsutil) (19.0.0)\nCollecting retry_decorator>=1.0.0\n  Downloading https://files.pythonhosted.org/packages/6e/e6/bedc75b264cbcbf6e6d0e5071d96d739f540fc09be31744a7a8824c02a8e/retry_decorator-1.1.1.tar.gz\nCollecting six>=1.16.0\n  Downloading https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl\nCollecting google-auth[aiohttp]>=2.5.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/8d/bff87fc722553a5691d8514da5523c23547f3894189ba03b57592e37bdc2/google_auth-2.22.0-py2.py3-none-any.whl (181kB)\n\u001b[K     |████████████████████████████████| 184kB 48.2MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: importlib-metadata<7,>=0.23; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from argcomplete>=1.9.4->gsutil) (1.3.0)\nCollecting rsa==4.7.2\n  Downloading https://files.pythonhosted.org/packages/e9/93/0c0f002031f18b53af7a6166103c02b9c0667be528944137cc954ec921b3/rsa-4.7.2-py3-none-any.whl\nRequirement already satisfied: boto>=2.29.1 in /opt/conda/lib/python3.6/site-packages (from gcs-oauth2-boto-plugin>=3.0->gsutil) (2.49.0)\nCollecting oauth2client>=2.2.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/a9/4f25a14d23f0786b64875b91784607c2277eff25d48f915e39ff0cff505a/oauth2client-4.1.3-py2.py3-none-any.whl (98kB)\n\u001b[K     |████████████████████████████████| 102kB 10.5MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > \"3.0\" in /opt/conda/lib/python3.6/site-packages (from httplib2==0.20.4->gsutil) (2.4.5)\nCollecting pyu2f\n  Downloading https://files.pythonhosted.org/packages/29/b5/c1209e6cb77647bc2c9a6a1a953355720f34f3b006b725e303c70f3c0786/pyu2f-0.1.5.tar.gz\nRequirement already satisfied: cryptography>=2.3 in /opt/conda/lib/python3.6/site-packages (from pyOpenSSL>=0.13->gsutil) (2.3.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth[aiohttp]>=2.5.0->gsutil) (4.0.0)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.6/site-packages (from google-auth[aiohttp]>=2.5.0->gsutil) (1.25.7)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth[aiohttp]>=2.5.0->gsutil) (0.2.8)\nRequirement already satisfied: requests<3.0.0.dev0,>=2.20.0; extra == \"aiohttp\" in /opt/conda/lib/python3.6/site-packages (from google-auth[aiohttp]>=2.5.0->gsutil) (2.22.0)\nCollecting aiohttp<4.0.0.dev0,>=3.6.2; extra == \"aiohttp\"\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/b6/6dd2c1f2915b39414f753c276f54dc2890612347a78e21c135fa3c76fd15/aiohttp-3.8.6-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (985kB)\n\u001b[K     |████████████████████████████████| 993kB 30.5MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata<7,>=0.23; python_version < \"3.8\"->argcomplete>=1.9.4->gsutil) (0.6.0)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa==4.7.2->gcs-oauth2-boto-plugin>=3.0->gsutil) (0.4.8)\nRequirement already satisfied: idna>=2.1 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.3->pyOpenSSL>=0.13->gsutil) (2.8)\nRequirement already satisfied: asn1crypto>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.3->pyOpenSSL>=0.13->gsutil) (1.2.0)\nRequirement already satisfied: cffi!=1.11.3,>=1.7 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.3->pyOpenSSL>=0.13->gsutil) (1.13.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0.dev0,>=2.20.0; extra == \"aiohttp\"->google-auth[aiohttp]>=2.5.0->gsutil) (2019.11.28)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0.dev0,>=2.20.0; extra == \"aiohttp\"->google-auth[aiohttp]>=2.5.0->gsutil) (3.0.4)\nCollecting async-timeout<5.0,>=4.0.0a3\n  Downloading https://files.pythonhosted.org/packages/d6/c1/8991e7c5385b897b8c020cdaad718c5b087a6626d1d11a23e1ea87e325a7/async_timeout-4.0.2-py3-none-any.whl\nCollecting idna-ssl>=1.0; python_version < \"3.7\"\n  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\nCollecting aiosignal>=1.1.2\n  Downloading https://files.pythonhosted.org/packages/3b/87/fe94898f2d44a93a35d5aa74671ed28094d80753a1113d68b799fab6dc22/aiosignal-1.2.0-py3-none-any.whl\nCollecting yarl<2.0,>=1.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/cb/8791922f5ec97b9ebec516d062c0e113da963568ffe2c7c04d8187ab7cc3/yarl-1.7.2-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (270kB)\n\u001b[K     |████████████████████████████████| 276kB 26.3MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp<4.0.0.dev0,>=3.6.2; extra == \"aiohttp\"->google-auth[aiohttp]>=2.5.0->gsutil) (19.3.0)\nCollecting asynctest==0.13.0; python_version < \"3.8\"\n  Downloading https://files.pythonhosted.org/packages/e8/b6/8d17e169d577ca7678b11cd0d3ceebb0a6089a7f4a2de4b945fe4b1c86db/asynctest-0.13.0-py3-none-any.whl\nCollecting multidict<7.0,>=4.5\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/43/81ddfbcfbdfaeaa0624f36dcb715dc8135562377b3292e93b0315a861e92/multidict-5.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (159kB)\n\u001b[K     |████████████████████████████████| 163kB 28.6MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from aiohttp<4.0.0.dev0,>=3.6.2; extra == \"aiohttp\"->google-auth[aiohttp]>=2.5.0->gsutil) (3.7.4.1)\nCollecting frozenlist>=1.1.1\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/3f/f67395ff0090b9f2835838a1f61c3e840baac70fd65bae762095dead48b2/frozenlist-1.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (191kB)\n\u001b[K     |████████████████████████████████| 194kB 47.7MB/s eta 0:00:01\n\u001b[?25hCollecting charset-normalizer<4.0,>=2.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/94/1725fc3e0dbe8918a4ec6dd317ec1ef388e701bdfb5053e1f34f5c6d5a8e/charset_normalizer-3.0.1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162kB)\n\u001b[K     |████████████████████████████████| 163kB 33.5MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: more-itertools in /opt/conda/lib/python3.6/site-packages (from zipp>=0.5->importlib-metadata<7,>=0.23; python_version < \"3.8\"->argcomplete>=1.9.4->gsutil) (8.0.2)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi!=1.11.3,>=1.7->cryptography>=2.3->pyOpenSSL>=0.13->gsutil) (2.19)\nBuilding wheels for collected packages: gsutil, crcmod, gcs-oauth2-boto-plugin, retry-decorator, pyu2f, idna-ssl\n  Building wheel for gsutil (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gsutil: filename=gsutil-5.27-cp36-none-any.whl size=3785289 sha256=afb56372e833254489bb7fb330d1ee96810498c76097a799969ab9ad050ca8e7\n  Stored in directory: /root/.cache/pip/wheels/78/ca/35/d5dc8238fbc968341f7ebecce34167572e352079f9e516aa6d\n  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp36-cp36m-linux_x86_64.whl size=35369 sha256=c5d6dc38f29952a4ec9ec01c70e9a3566feed0b40faef963c83808d8c1656f6e\n  Stored in directory: /root/.cache/pip/wheels/50/24/4d/4580ca4a299f1ad6fd63443e6e584cb21e9a07988e4aa8daac\n  Building wheel for gcs-oauth2-boto-plugin (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gcs-oauth2-boto-plugin: filename=gcs_oauth2_boto_plugin-3.0-cp36-none-any.whl size=23203 sha256=262ffaa21a745263aaacfc28b99d9224c55a0ffccc02abebceb6689c3b252af2\n  Stored in directory: /root/.cache/pip/wheels/b5/36/e4/4b9f9e63a2e60da009ff3f46e4eeb9d83ac0f8ae65fa6dc198\n  Building wheel for retry-decorator (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for retry-decorator: filename=retry_decorator-1.1.1-py2.py3-none-any.whl size=3639 sha256=783f0772d523d536f5b0380451d8e5dbad6f3dfb26cbab16d2aa5a9082600e2a\n  Stored in directory: /root/.cache/pip/wheels/a1/70/30/4af820545aa19a0d96f969ef5ecebbb9743fd89cf00db43273\n  Building wheel for pyu2f (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyu2f: filename=pyu2f-0.1.5-cp36-none-any.whl size=39388 sha256=37a4f725c205b20e8db758b895742af9b7ed6c3630603f19153b9c5e27e446d0\n  Stored in directory: /root/.cache/pip/wheels/b9/74/4d/2a07cf37327596c99f570ebe983a9843cda0278ca36a27ad9d\n  Building wheel for idna-ssl (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3163 sha256=47f28ca24dabff0af6a054f9a3c4fe4e700f0235befbc87e4f53e3a88bd18e8c\n  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\nSuccessfully built gsutil crcmod gcs-oauth2-boto-plugin retry-decorator pyu2f idna-ssl\n\u001b[31mERROR: allennlp 0.9.0 requires flaky, which is not installed.\u001b[0m\n\u001b[31mERROR: allennlp 0.9.0 requires responses>=0.7, which is not installed.\u001b[0m\n\u001b[31mERROR: tensorflow-probability 0.8.0 has requirement cloudpickle==1.1.1, but you'll have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n\u001b[31mERROR: tensorboard 2.1.0 has requirement google-auth<2,>=1.6.3, but you'll have google-auth 2.22.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n\u001b[31mERROR: hyperopt 0.2.2 has requirement networkx==2.2, but you'll have networkx 2.4 which is incompatible.\u001b[0m\n\u001b[31mERROR: gql 0.2.0 has requirement graphql-core<2,>=0.5.0, but you'll have graphql-core 2.2.1 which is incompatible.\u001b[0m\n\u001b[31mERROR: google-cloud-storage 1.24.1 has requirement google-auth<2.0dev,>=1.9.0, but you'll have google-auth 2.22.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: google-api-core 1.15.0 has requirement google-auth<2.0dev,>=0.4.0, but you'll have google-auth 2.22.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: docs 0.2.2 has requirement networkx==2.2, but you'll have networkx 2.4 which is incompatible.\u001b[0m\n\u001b[31mERROR: allennlp 0.9.0 has requirement spacy<2.2,>=2.1.0, but you'll have spacy 2.2.3 which is incompatible.\u001b[0m\nInstalling collected packages: argcomplete, crcmod, rsa, six, pyu2f, google-reauth, httplib2, oauth2client, retry-decorator, gcs-oauth2-boto-plugin, google-apitools, async-timeout, idna-ssl, frozenlist, aiosignal, multidict, yarl, asynctest, charset-normalizer, aiohttp, google-auth, gsutil\n  Found existing installation: rsa 4.0\n    Uninstalling rsa-4.0:\n      Successfully uninstalled rsa-4.0\n  Found existing installation: six 1.13.0\n    Uninstalling six-1.13.0:\n      Successfully uninstalled six-1.13.0\n  Found existing installation: httplib2 0.15.0\n    Uninstalling httplib2-0.15.0:\n      Successfully uninstalled httplib2-0.15.0\n  Found existing installation: google-auth 1.10.0\n    Uninstalling google-auth-1.10.0:\n      Successfully uninstalled google-auth-1.10.0\nSuccessfully installed aiohttp-3.8.6 aiosignal-1.2.0 argcomplete-3.1.2 async-timeout-4.0.2 asynctest-0.13.0 charset-normalizer-3.0.1 crcmod-1.7 frozenlist-1.2.0 gcs-oauth2-boto-plugin-3.0 google-apitools-0.5.32 google-auth-2.22.0 google-reauth-0.1.1 gsutil-5.27 httplib2-0.20.4 idna-ssl-1.1.0 multidict-5.2.0 oauth2client-4.1.3 pyu2f-0.1.5 retry-decorator-1.1.1 rsa-4.7.2 six-1.16.0 yarl-1.7.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!gsutil -m cp -r \"gs://quickdraw_dataset/full/numpy_bitmap/camel.npy\" .","metadata":{"execution":{"iopub.status.busy":"2024-03-19T04:07:11.550693Z","iopub.execute_input":"2024-03-19T04:07:11.551051Z","iopub.status.idle":"2024-03-19T04:07:15.181757Z","shell.execute_reply.started":"2024-03-19T04:07:11.551003Z","shell.execute_reply":"2024-03-19T04:07:15.180221Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Copying gs://quickdraw_dataset/full/numpy_bitmap/camel.npy...\n\\ [1/1 files][ 90.8 MiB/ 90.8 MiB] 100% Done                                    \nOperation completed over 1 objects/90.8 MiB.                                     \n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir gan","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:26:55.251844Z","iopub.execute_input":"2024-03-19T03:26:55.252473Z","iopub.status.idle":"2024-03-19T03:26:56.253987Z","shell.execute_reply.started":"2024-03-19T03:26:55.252189Z","shell.execute_reply":"2024-03-19T03:26:56.252900Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"cd gan","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:27:01.166355Z","iopub.execute_input":"2024-03-19T03:27:01.166969Z","iopub.status.idle":"2024-03-19T03:27:01.174934Z","shell.execute_reply.started":"2024-03-19T03:27:01.166875Z","shell.execute_reply":"2024-03-19T03:27:01.173884Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/working/GDL_code/gan\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\nfrom keras.layers.merge import _Merge\n\nfrom keras.models import Model, Sequential\nfrom keras import backend as K\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.utils import plot_model\nfrom keras.initializers import RandomNormal\n\nimport numpy as np\nimport json\nimport os\nimport pickle as pkl\nimport matplotlib.pyplot as plt\n\n\nclass GAN():\n    def __init__(self\n        , input_dim\n        , discriminator_conv_filters\n        , discriminator_conv_kernel_size\n        , discriminator_conv_strides\n        , discriminator_batch_norm_momentum\n        , discriminator_activation\n        , discriminator_dropout_rate\n        , discriminator_learning_rate\n        , generator_initial_dense_layer_size\n        , generator_upsample\n        , generator_conv_filters\n        , generator_conv_kernel_size\n        , generator_conv_strides\n        , generator_batch_norm_momentum\n        , generator_activation\n        , generator_dropout_rate\n        , generator_learning_rate\n        , optimiser\n        , z_dim\n        ):\n\n        self.name = 'gan'\n\n        self.input_dim = input_dim\n        self.discriminator_conv_filters = discriminator_conv_filters\n        self.discriminator_conv_kernel_size = discriminator_conv_kernel_size\n        self.discriminator_conv_strides = discriminator_conv_strides\n        self.discriminator_batch_norm_momentum = discriminator_batch_norm_momentum\n        self.discriminator_activation = discriminator_activation\n        self.discriminator_dropout_rate = discriminator_dropout_rate\n        self.discriminator_learning_rate = discriminator_learning_rate\n\n        self.generator_initial_dense_layer_size = generator_initial_dense_layer_size\n        self.generator_upsample = generator_upsample\n        self.generator_conv_filters = generator_conv_filters\n        self.generator_conv_kernel_size = generator_conv_kernel_size\n        self.generator_conv_strides = generator_conv_strides\n        self.generator_batch_norm_momentum = generator_batch_norm_momentum\n        self.generator_activation = generator_activation\n        self.generator_dropout_rate = generator_dropout_rate\n        self.generator_learning_rate = generator_learning_rate\n        \n        self.optimiser = optimiser\n        self.z_dim = z_dim\n\n        self.n_layers_discriminator = len(discriminator_conv_filters)\n        self.n_layers_generator = len(generator_conv_filters)\n\n        self.weight_init = RandomNormal(mean=0., stddev=0.02)\n\n        self.d_losses = []\n        self.g_losses = []\n\n        self.epoch = 0\n\n        self._build_discriminator()\n        self._build_generator()\n\n        self._build_adversarial()\n\n    def get_activation(self, activation):\n        if activation == 'leaky_relu':\n            layer = LeakyReLU(alpha = 0.2)\n        else:\n            layer = Activation(activation)\n        return layer\n\n    def _build_discriminator(self):\n\n        ### THE discriminator\n        discriminator_input = Input(shape=self.input_dim, name='discriminator_input')\n\n        x = discriminator_input\n\n        for i in range(self.n_layers_discriminator):\n\n            x = Conv2D(\n                filters = self.discriminator_conv_filters[i]\n                , kernel_size = self.discriminator_conv_kernel_size[i]\n                , strides = self.discriminator_conv_strides[i]\n                , padding = 'same'\n                , name = 'discriminator_conv_' + str(i)\n                , kernel_initializer = self.weight_init\n                )(x)\n\n            if self.discriminator_batch_norm_momentum and i > 0:\n                x = BatchNormalization(momentum = self.discriminator_batch_norm_momentum)(x)\n\n            x = self.get_activation(self.discriminator_activation)(x)\n\n            if self.discriminator_dropout_rate:\n                x = Dropout(rate = self.discriminator_dropout_rate)(x)\n\n        x = Flatten()(x)\n        \n        discriminator_output = Dense(1, activation='sigmoid', kernel_initializer = self.weight_init)(x)\n\n        self.discriminator = Model(discriminator_input, discriminator_output)\n\n\n    def _build_generator(self):\n\n        ### THE generator\n\n        generator_input = Input(shape=(self.z_dim,), name='generator_input')\n\n        x = generator_input\n\n        x = Dense(np.prod(self.generator_initial_dense_layer_size), kernel_initializer = self.weight_init)(x)\n\n        if self.generator_batch_norm_momentum:\n            x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n\n        x = self.get_activation(self.generator_activation)(x)\n\n        x = Reshape(self.generator_initial_dense_layer_size)(x)\n\n        if self.generator_dropout_rate:\n            x = Dropout(rate = self.generator_dropout_rate)(x)\n\n        for i in range(self.n_layers_generator):\n\n            if self.generator_upsample[i] == 2:\n                x = UpSampling2D()(x)\n                x = Conv2D(\n                    filters = self.generator_conv_filters[i]\n                    , kernel_size = self.generator_conv_kernel_size[i]\n                    , padding = 'same'\n                    , name = 'generator_conv_' + str(i)\n                    , kernel_initializer = self.weight_init\n                )(x)\n            else:\n\n                x = Conv2DTranspose(\n                    filters = self.generator_conv_filters[i]\n                    , kernel_size = self.generator_conv_kernel_size[i]\n                    , padding = 'same'\n                    , strides = self.generator_conv_strides[i]\n                    , name = 'generator_conv_' + str(i)\n                    , kernel_initializer = self.weight_init\n                    )(x)\n\n            if i < self.n_layers_generator - 1:\n\n                if self.generator_batch_norm_momentum:\n                    x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n\n                x = self.get_activation(self.generator_activation)(x)\n                    \n                \n            else:\n\n                x = Activation('tanh')(x)\n\n\n        generator_output = x\n\n        self.generator = Model(generator_input, generator_output)\n\n       \n    def get_opti(self, lr):\n        if self.optimiser == 'adam':\n            opti = Adam(lr=lr, beta_1=0.5)\n        elif self.optimiser == 'rmsprop':\n            opti = RMSprop(lr=lr)\n        else:\n            opti = Adam(lr=lr)\n\n        return opti\n\n    def set_trainable(self, m, val):\n        m.trainable = val\n        for l in m.layers:\n            l.trainable = val\n\n\n    def _build_adversarial(self):\n        \n        ### COMPILE DISCRIMINATOR\n\n        self.discriminator.compile(\n        optimizer=self.get_opti(self.discriminator_learning_rate)  \n        , loss = 'binary_crossentropy'\n        ,  metrics = ['accuracy']\n        )\n        \n        ### COMPILE THE FULL GAN\n\n        self.set_trainable(self.discriminator, False)\n\n        model_input = Input(shape=(self.z_dim,), name='model_input')\n        model_output = self.discriminator(self.generator(model_input))\n        self.model = Model(model_input, model_output)\n\n        self.model.compile(optimizer=self.get_opti(self.generator_learning_rate) , loss='binary_crossentropy', metrics=['accuracy'])\n\n        self.set_trainable(self.discriminator, True)\n\n\n\n    \n    def train_discriminator(self, x_train, batch_size, using_generator):\n\n        valid = np.ones((batch_size,1))\n        fake = np.zeros((batch_size,1))\n\n        if using_generator:\n            true_imgs = next(x_train)[0]\n            if true_imgs.shape[0] != batch_size:\n                true_imgs = next(x_train)[0]\n        else:\n            idx = np.random.randint(0, x_train.shape[0], batch_size)\n            true_imgs = x_train[idx]\n        \n        noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n        gen_imgs = self.generator.predict(noise)\n\n        d_loss_real, d_acc_real =   self.discriminator.train_on_batch(true_imgs, valid)\n        d_loss_fake, d_acc_fake =   self.discriminator.train_on_batch(gen_imgs, fake)\n        d_loss =  0.5 * (d_loss_real + d_loss_fake)\n        d_acc = 0.5 * (d_acc_real + d_acc_fake)\n\n        return [d_loss, d_loss_real, d_loss_fake, d_acc, d_acc_real, d_acc_fake]\n\n    def train_generator(self, batch_size):\n        valid = np.ones((batch_size,1))\n        noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n        return self.model.train_on_batch(noise, valid)\n\n\n    def train(self, x_train, batch_size, epochs, run_folder\n    , print_every_n_batches = 50\n    , using_generator = False):\n\n        for epoch in range(self.epoch, self.epoch + epochs):\n\n            d = self.train_discriminator(x_train, batch_size, using_generator)\n            g = self.train_generator(batch_size)\n\n            print (\"%d [D loss: (%.3f)(R %.3f, F %.3f)] [D acc: (%.3f)(%.3f, %.3f)] [G loss: %.3f] [G acc: %.3f]\" % (epoch, d[0], d[1], d[2], d[3], d[4], d[5], g[0], g[1]))\n\n            self.d_losses.append(d)\n            self.g_losses.append(g)\n\n            if epoch % print_every_n_batches == 0:\n                self.sample_images(run_folder)\n                self.model.save_weights(os.path.join(run_folder, 'weights/weights-%d.h5' % (epoch)))\n                self.model.save_weights(os.path.join(run_folder, 'weights/weights.h5'))\n                self.save_model(run_folder)\n\n            self.epoch += 1\n\n    \n    def sample_images(self, run_folder):\n        r, c = 5, 5\n        noise = np.random.normal(0, 1, (r * c, self.z_dim))\n        gen_imgs = self.generator.predict(noise)\n\n        gen_imgs = 0.5 * (gen_imgs + 1)\n        gen_imgs = np.clip(gen_imgs, 0, 1)\n\n        fig, axs = plt.subplots(r, c, figsize=(15,15))\n        cnt = 0\n\n        for i in range(r):\n            for j in range(c):\n                axs[i,j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]), cmap = 'gray')\n                axs[i,j].axis('off')\n                cnt += 1\n        fig.savefig(os.path.join(run_folder, \"images/sample_%d.png\" % self.epoch))\n        plt.close()\n\n\n\n\n    \n    def plot_model(self, run_folder):\n        plot_model(self.model, to_file=os.path.join(run_folder ,'viz/model.png'), show_shapes = True, show_layer_names = True)\n        plot_model(self.discriminator, to_file=os.path.join(run_folder ,'viz/discriminator.png'), show_shapes = True, show_layer_names = True)\n        plot_model(self.generator, to_file=os.path.join(run_folder ,'viz/generator.png'), show_shapes = True, show_layer_names = True)\n\n\n\n    def save(self, folder):\n\n        with open(os.path.join(folder, 'params.pkl'), 'wb') as f:\n            pkl.dump([\n                self.input_dim\n                , self.discriminator_conv_filters\n                , self.discriminator_conv_kernel_size\n                , self.discriminator_conv_strides\n                , self.discriminator_batch_norm_momentum\n                , self.discriminator_activation\n                , self.discriminator_dropout_rate\n                , self.discriminator_learning_rate\n                , self.generator_initial_dense_layer_size\n                , self.generator_upsample\n                , self.generator_conv_filters\n                , self.generator_conv_kernel_size\n                , self.generator_conv_strides\n                , self.generator_batch_norm_momentum\n                , self.generator_activation\n                , self.generator_dropout_rate\n                , self.generator_learning_rate\n                , self.optimiser\n                , self.z_dim\n                ], f)\n\n        self.plot_model(folder)\n\n    def save_model(self, run_folder):\n        self.model.save(os.path.join(run_folder, 'model.h5'))\n        self.discriminator.save(os.path.join(run_folder, 'discriminator.h5'))\n        self.generator.save(os.path.join(run_folder, 'generator.h5'))\n        pkl.dump(self, open( os.path.join(run_folder, \"obj.pkl\"), \"wb\" ))\n\n    def load_weights(self, filepath):\n        self.model.load_weights(filepath)\n\n\n        \n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:27:05.631194Z","iopub.execute_input":"2024-03-19T03:27:05.631845Z","iopub.status.idle":"2024-03-19T03:27:12.996499Z","shell.execute_reply.started":"2024-03-19T03:27:05.631790Z","shell.execute_reply":"2024-03-19T03:27:12.995362Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nimport os\n\nfrom keras.datasets import mnist, cifar100,cifar10\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, save_img, img_to_array\n\nimport pandas as pd\n\nimport numpy as np\nfrom os import walk, getcwd\nimport h5py\n\nimport scipy\nfrom glob import glob\n\nfrom keras.applications import vgg19\nfrom keras import backend as K\nfrom keras.utils import to_categorical\n\nimport pdb\n\n\nclass ImageLabelLoader():\n    def __init__(self, image_folder, target_size):\n        self.image_folder = image_folder\n        self.target_size = target_size\n\n    def build(self, att, batch_size, label = None):\n\n        data_gen = ImageDataGenerator(rescale=1./255)\n        if label:\n            data_flow = data_gen.flow_from_dataframe(\n                att\n                , self.image_folder\n                , x_col='image_id'\n                , y_col=label\n                , target_size=self.target_size \n                , class_mode='other'\n                , batch_size=batch_size\n                , shuffle=True\n            )\n        else:\n            data_flow = data_gen.flow_from_dataframe(\n                att\n                , self.image_folder\n                , x_col='image_id'\n                , target_size=self.target_size \n                , class_mode='input'\n                , batch_size=batch_size\n                , shuffle=True\n            )\n\n        return data_flow\n\n\n\n\nclass DataLoader():\n    def __init__(self, dataset_name, img_res=(256, 256)):\n        self.dataset_name = dataset_name\n        self.img_res = img_res\n\n    def load_data(self, domain, batch_size=1, is_testing=False):\n        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n        path = glob('./data/%s/%s/*' % (self.dataset_name, data_type))\n\n        batch_images = np.random.choice(path, size=batch_size)\n\n        imgs = []\n        for img_path in batch_images:\n            img = self.imread(img_path)\n            if not is_testing:\n                img = scipy.misc.imresize(img, self.img_res)\n\n                if np.random.random() > 0.5:\n                    img = np.fliplr(img)\n            else:\n                img = scipy.misc.imresize(img, self.img_res)\n            imgs.append(img)\n\n        imgs = np.array(imgs)/127.5 - 1.\n\n        return imgs\n\n    def load_batch(self, batch_size=1, is_testing=False):\n        data_type = \"train\" if not is_testing else \"val\"\n        path_A = glob('./data/%s/%sA/*' % (self.dataset_name, data_type))\n        path_B = glob('./data/%s/%sB/*' % (self.dataset_name, data_type))\n\n        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n        total_samples = self.n_batches * batch_size\n\n        # Sample n_batches * batch_size from each path list so that model sees all\n        # samples from both domains\n        path_A = np.random.choice(path_A, total_samples, replace=False)\n        path_B = np.random.choice(path_B, total_samples, replace=False)\n\n        for i in range(self.n_batches-1):\n            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n            imgs_A, imgs_B = [], []\n            for img_A, img_B in zip(batch_A, batch_B):\n                img_A = self.imread(img_A)\n                img_B = self.imread(img_B)\n\n                img_A = scipy.misc.imresize(img_A, self.img_res)\n                img_B = scipy.misc.imresize(img_B, self.img_res)\n\n                if not is_testing and np.random.random() > 0.5:\n                        img_A = np.fliplr(img_A)\n                        img_B = np.fliplr(img_B)\n\n                imgs_A.append(img_A)\n                imgs_B.append(img_B)\n\n            imgs_A = np.array(imgs_A)/127.5 - 1.\n            imgs_B = np.array(imgs_B)/127.5 - 1.\n\n            yield imgs_A, imgs_B\n\n    def load_img(self, path):\n        img = self.imread(path)\n        img = scipy.misc.imresize(img, self.img_res)\n        img = img/127.5 - 1.\n        return img[np.newaxis, :, :, :]\n\n    def imread(self, path):\n        return scipy.misc.imread(path, mode='RGB').astype(np.float)\n\n\n\n\ndef load_model(model_class, folder):\n    \n    with open(os.path.join(folder, 'params.pkl'), 'rb') as f:\n        params = pickle.load(f)\n\n    model = model_class(*params)\n\n    model.load_weights(os.path.join(folder, 'weights/weights.h5'))\n\n    return model\n\n\ndef load_mnist():\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n    x_train = x_train.astype('float32') / 255.\n    x_train = x_train.reshape(x_train.shape + (1,))\n    x_test = x_test.astype('float32') / 255.\n    x_test = x_test.reshape(x_test.shape + (1,))\n\n    return (x_train, y_train), (x_test, y_test)\n\ndef load_mnist_gan():\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n    x_train = (x_train.astype('float32') - 127.5) / 127.5\n    x_train = x_train.reshape(x_train.shape + (1,))\n    x_test = (x_test.astype('float32') - 127.5) / 127.5\n    x_test = x_test.reshape(x_test.shape + (1,))\n\n    return (x_train, y_train), (x_test, y_test)\n\n\n\ndef load_fashion_mnist(input_rows, input_cols, path='./data/fashion/fashion-mnist_train.csv'):\n    #read the csv data\n    df = pd.read_csv(path)\n    #extract the image pixels\n    X_train = df.drop(columns = ['label'])\n    X_train = X_train.values\n    X_train = (X_train.astype('float32') - 127.5) / 127.5\n    X_train = X_train.reshape(X_train.shape[0], input_rows, input_cols, 1)\n    #extract the labels\n    y_train = df['label'].values\n    \n    return X_train, y_train\n\ndef load_safari(folder):\n\n    mypath = os.path.join(\"./data\", folder)\n    txt_name_list = []\n    for (dirpath, dirnames, filenames) in walk(mypath):\n        for f in filenames:\n            if f != '.DS_Store':\n                txt_name_list.append(f)\n                break\n\n    slice_train = int(80000/len(txt_name_list))  ###Setting value to be 80000 for the final dataset\n    i = 0\n    seed = np.random.randint(1, 10e6)\n\n    for txt_name in txt_name_list:\n        txt_path = os.path.join(mypath,txt_name)\n        x = np.load(txt_path)\n        x = (x.astype('float32') - 127.5) / 127.5\n        # x = x.astype('float32') / 255.0\n        \n        x = x.reshape(x.shape[0], 28, 28, 1)\n        \n        y = [i] * len(x)  \n        np.random.seed(seed)\n        np.random.shuffle(x)\n        np.random.seed(seed)\n        np.random.shuffle(y)\n        x = x[:slice_train]\n        y = y[:slice_train]\n        if i != 0: \n            xtotal = np.concatenate((x,xtotal), axis=0)\n            ytotal = np.concatenate((y,ytotal), axis=0)\n        else:\n            xtotal = x\n            ytotal = y\n        i += 1\n        \n    return xtotal, ytotal\n\n\n\ndef load_cifar(label, num):\n    if num == 10:\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    else:\n        (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode = 'fine')\n\n    train_mask = [y[0]==label for y in y_train]\n    test_mask = [y[0]==label for y in y_test]\n\n    x_data = np.concatenate([x_train[train_mask], x_test[test_mask]])\n    y_data = np.concatenate([y_train[train_mask], y_test[test_mask]])\n\n    x_data = (x_data.astype('float32') - 127.5) / 127.5\n \n    return (x_data, y_data)\n\n\ndef load_celeb(data_name, image_size, batch_size):\n    data_folder = os.path.join(\"./data\", data_name)\n\n    data_gen = ImageDataGenerator(preprocessing_function=lambda x: (x.astype('float32') - 127.5) / 127.5)\n\n    x_train = data_gen.flow_from_directory(data_folder\n                                            , target_size = (image_size,image_size)\n                                            , batch_size = batch_size\n                                            , shuffle = True\n                                            , class_mode = 'input'\n                                            , subset = \"training\"\n                                                )\n\n    return x_train\n\n\ndef load_music(data_name, filename, n_bars, n_steps_per_bar):\n    file = os.path.join(\"./data\", data_name, filename)\n\n    with np.load(file, encoding='bytes') as f:\n        data = f['train']\n\n    data_ints = []\n\n    for x in data:\n        counter = 0\n        cont = True\n        while cont:\n            if not np.any(np.isnan(x[counter:(counter+4)])):\n                cont = False\n            else:\n                counter += 4\n\n        if n_bars * n_steps_per_bar < x.shape[0]:\n            data_ints.append(x[counter:(counter + (n_bars * n_steps_per_bar)),:])\n\n\n    data_ints = np.array(data_ints)\n\n    n_songs = data_ints.shape[0]\n    n_tracks = data_ints.shape[2]\n\n    data_ints = data_ints.reshape([n_songs, n_bars, n_steps_per_bar, n_tracks])\n\n    max_note = 83\n\n    where_are_NaNs = np.isnan(data_ints)\n    data_ints[where_are_NaNs] = max_note + 1\n    max_note = max_note + 1\n\n    data_ints = data_ints.astype(int)\n\n    num_classes = max_note + 1\n\n    \n    data_binary = np.eye(num_classes)[data_ints]\n    data_binary[data_binary==0] = -1\n    data_binary = np.delete(data_binary, max_note,-1)\n\n    data_binary = data_binary.transpose([0,1,2, 4,3])\n    \n    \n\n    \n\n    return data_binary, data_ints, data\n\n\ndef preprocess_image(data_name, file, img_nrows, img_ncols):\n\n    image_path = os.path.join('./data', data_name, file)\n\n    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = vgg19.preprocess_input(img)\n    return img\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:27:44.967683Z","iopub.execute_input":"2024-03-19T03:27:44.968311Z","iopub.status.idle":"2024-03-19T03:27:45.030866Z","shell.execute_reply.started":"2024-03-19T03:27:44.968235Z","shell.execute_reply":"2024-03-19T03:27:45.030085Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:28:01.517830Z","iopub.execute_input":"2024-03-19T03:28:01.518429Z","iopub.status.idle":"2024-03-19T03:28:01.523419Z","shell.execute_reply.started":"2024-03-19T03:28:01.518364Z","shell.execute_reply":"2024-03-19T03:28:01.522439Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"cd ..","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:29:00.918242Z","iopub.execute_input":"2024-03-19T03:29:00.918581Z","iopub.status.idle":"2024-03-19T03:29:00.924378Z","shell.execute_reply.started":"2024-03-19T03:29:00.918541Z","shell.execute_reply":"2024-03-19T03:29:00.923281Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"/kaggle/working/GDL_code\n","output_type":"stream"}]},{"cell_type":"code","source":"cd /kaggle/working/GDL_code/data","metadata":{"execution":{"iopub.status.busy":"2024-03-19T04:09:08.324439Z","iopub.execute_input":"2024-03-19T04:09:08.324889Z","iopub.status.idle":"2024-03-19T04:09:08.332523Z","shell.execute_reply.started":"2024-03-19T04:09:08.324816Z","shell.execute_reply":"2024-03-19T04:09:08.331397Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"/kaggle/working/GDL_code/data\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir camel","metadata":{"execution":{"iopub.status.busy":"2024-03-19T04:09:19.851821Z","iopub.execute_input":"2024-03-19T04:09:19.852198Z","iopub.status.idle":"2024-03-19T04:09:20.874920Z","shell.execute_reply.started":"2024-03-19T04:09:19.852134Z","shell.execute_reply":"2024-03-19T04:09:20.873636Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"cd ..","metadata":{"execution":{"iopub.status.busy":"2024-03-19T04:09:25.381204Z","iopub.execute_input":"2024-03-19T04:09:25.381575Z","iopub.status.idle":"2024-03-19T04:09:25.388349Z","shell.execute_reply.started":"2024-03-19T04:09:25.381521Z","shell.execute_reply":"2024-03-19T04:09:25.387299Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"/kaggle/working/GDL_code\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\nshutil.move('/kaggle/working/GDL_code/camel.npy','/kaggle/working/GDL_code/data/camel')","metadata":{"execution":{"iopub.status.busy":"2024-03-19T04:10:05.500314Z","iopub.execute_input":"2024-03-19T04:10:05.500945Z","iopub.status.idle":"2024-03-19T04:10:05.507454Z","shell.execute_reply.started":"2024-03-19T04:10:05.500869Z","shell.execute_reply":"2024-03-19T04:10:05.506470Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/GDL_code/data/camel/camel.npy'"},"metadata":{}}]},{"cell_type":"code","source":"# run params\nSECTION = 'gan'\nRUN_ID = '0001'\nDATA_NAME = 'camel'\nRUN_FOLDER = 'run/{}/'.format(SECTION)\nRUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n\nif not os.path.exists(RUN_FOLDER):\n    os.mkdir(RUN_FOLDER)\n    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n\nmode =  'build' #'load' #","metadata":{"execution":{"iopub.status.busy":"2024-03-19T04:10:12.151595Z","iopub.execute_input":"2024-03-19T04:10:12.152008Z","iopub.status.idle":"2024-03-19T04:10:12.160445Z","shell.execute_reply.started":"2024-03-19T04:10:12.151931Z","shell.execute_reply":"2024-03-19T04:10:12.159167Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"(x_train, y_train) = load_safari(DATA_NAME)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T04:10:19.162092Z","iopub.execute_input":"2024-03-19T04:10:19.162448Z","iopub.status.idle":"2024-03-19T04:10:20.152791Z","shell.execute_reply.started":"2024-03-19T04:10:19.162397Z","shell.execute_reply":"2024-03-19T04:10:20.151660Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working/GDL_code","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:51:37.164497Z","iopub.execute_input":"2024-03-19T03:51:37.164876Z","iopub.status.idle":"2024-03-19T03:51:37.171199Z","shell.execute_reply.started":"2024-03-19T03:51:37.164820Z","shell.execute_reply":"2024-03-19T03:51:37.170227Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"/kaggle/working/GDL_code\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('camel.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T03:50:12.399392Z","iopub.execute_input":"2024-03-19T03:50:12.399895Z","iopub.status.idle":"2024-03-19T03:50:13.877076Z","shell.execute_reply.started":"2024-03-19T03:50:12.399827Z","shell.execute_reply":"2024-03-19T03:50:13.876107Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"  countrycode                                            drawing  \\\n0          AU  [[[47, 27, 15, 3, 0, 2, 12, 33, 46, 51, 53, 49...   \n1          PH  [[[62, 63, 72, 82, 98, 109, 123, 130, 136], [7...   \n2          SA  [[[36, 15, 10, 0, 1, 21, 33, 39, 41], [85, 85,...   \n3          IL  [[[0, 16, 25, 42, 60, 68, 88, 108, 139, 159, 1...   \n4          HK  [[[183, 192, 205, 225, 232, 239, 255, 251, 243...   \n\n             key_id  recognized                   timestamp   word  \n0  4652244525907968        True  2017-03-09 09:13:23.708210  camel  \n1  4641031645560832        True  2017-01-26 03:02:58.055830  camel  \n2  6202541207453696        True  2017-03-18 12:45:05.278380  camel  \n3  6110810722009088        True  2017-03-13 14:17:50.754560  camel  \n4  5519999802277888        True  2017-03-25 15:07:51.055470  camel  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>countrycode</th>\n      <th>drawing</th>\n      <th>key_id</th>\n      <th>recognized</th>\n      <th>timestamp</th>\n      <th>word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AU</td>\n      <td>[[[47, 27, 15, 3, 0, 2, 12, 33, 46, 51, 53, 49...</td>\n      <td>4652244525907968</td>\n      <td>True</td>\n      <td>2017-03-09 09:13:23.708210</td>\n      <td>camel</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PH</td>\n      <td>[[[62, 63, 72, 82, 98, 109, 123, 130, 136], [7...</td>\n      <td>4641031645560832</td>\n      <td>True</td>\n      <td>2017-01-26 03:02:58.055830</td>\n      <td>camel</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SA</td>\n      <td>[[[36, 15, 10, 0, 1, 21, 33, 39, 41], [85, 85,...</td>\n      <td>6202541207453696</td>\n      <td>True</td>\n      <td>2017-03-18 12:45:05.278380</td>\n      <td>camel</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>IL</td>\n      <td>[[[0, 16, 25, 42, 60, 68, 88, 108, 139, 159, 1...</td>\n      <td>6110810722009088</td>\n      <td>True</td>\n      <td>2017-03-13 14:17:50.754560</td>\n      <td>camel</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HK</td>\n      <td>[[[183, 192, 205, 225, 232, 239, 255, 251, 243...</td>\n      <td>5519999802277888</td>\n      <td>True</td>\n      <td>2017-03-25 15:07:51.055470</td>\n      <td>camel</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"plt.imshow(x_train[200,:,:,0], cmap = 'gray')","metadata":{"execution":{"iopub.status.busy":"2024-03-19T04:11:49.820149Z","iopub.execute_input":"2024-03-19T04:11:49.820790Z","iopub.status.idle":"2024-03-19T04:11:50.045936Z","shell.execute_reply.started":"2024-03-19T04:11:49.820728Z","shell.execute_reply":"2024-03-19T04:11:50.044698Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7dd7230daa90>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEOpJREFUeJzt3XuMldW9xvHnJ1KDtGPEy4gUD7WBcgzG22iMoqIGBOMNoygmBpOmNFKiJl6O8IegxKgntkqMlo5xFAlFa1qVqKBoTFBUYERTsGhV5CDHATSYgIBcf+cPNiejzvt7N/s+rO8naWZmP3vNXt3jw7tn1n7fZe4uAOk5qN4TAFAflB9IFOUHEkX5gURRfiBRlB9IFOUHEkX5gURRfiBRB9fywcyMtxMCVebuVsz9yjrym9lIM/vEzD4zszvL+V4AastKfW+/mfWQ9G9JwyWtlbRU0lh3/1cwhiM/UGW1OPKfIekzd1/l7jskPSPp8jK+H4AaKqf8/SR92enrtYXbfsDMxptZu5m1l/FYACqsnD/4dfXS4icv6929VVKrxMt+oJGUc+RfK6l/p69/Kemr8qYDoFbKKf9SSQPN7Fdm9jNJ10qaW5lpAai2kl/2u/suM5so6VVJPSS1uftHFZsZgKoqeamvpAfjd36g6mryJh8A3RflBxJF+YFEUX4gUZQfSBTlBxJV0/P5gVoaMmRIZnbppZeGYw877LCyHvv1118vK68FjvxAoig/kCjKDySK8gOJovxAoig/kKhkzuq76qqrwvy8884L8z59+mRmTU1N4di33347zB9++OEw3759e5iXY8CAAWF+xx13hPlJJ50U5occckhmdsEFF4Rjt27dGuazZs0K82uvvTYz27VrVzh28+bNYd6zZ88w7927d5hfeeWVmdkLL7wQjs3DWX0AQpQfSBTlBxJF+YFEUX4gUZQfSBTlBxLVrdb5m5ubM7OnnnoqHDty5MgwX7duXZh/8803mVneOvwpp5wS5kuXLg3z0aNHh3lHR0dmlvcehHfffTfM+/fvH+bt7fEubOeff35mNmHChHBs3lr69OnTw3zSpEmZWWtrazh248aNYX7wwfHZ8G+++WaYDxo0KDPLe8537NgR5qzzAwhRfiBRlB9IFOUHEkX5gURRfiBRlB9IVFnr/Ga2WtJmSbsl7XL3lpz7l7XO/8QTT2RmY8aMCcfecsstYd7W1hbm5TxPo0aNCvM5c+aE+ZIlS8L8oosuyszuvffecOztt98e5uecc06Yv/fee2H+ySefZGYffPBBOPb0008P81WrVoX58OHDw7ya8n7mr7zySmY2dOjQcOyiRYvCvNh1/kpct/98d89+BwyAhsTLfiBR5ZbfJb1mZu+b2fhKTAhAbZT7sv9sd//KzI6WtMDMPnb3hZ3vUPhHgX8YgAZT1pHf3b8qfNwg6XlJZ3Rxn1Z3b8n7YyCA2iq5/GbW28x+se9zSSMkrajUxABUVzkv+5slPW9m+77PX919fkVmBaDqSi6/u6+SFF+0fT8dccQRYT527NjM7C9/+Us4NnqPQLXNmzcvzPPeg/Dkk0+G+dSpUzOziRMnhmNnz54d5nnr+Hk2bdqUmeVdYyFvm+y8/2/1tGzZspLH5l3/IW+dv1gs9QGJovxAoig/kCjKDySK8gOJovxAoipxVl/F5G2j3atXr8xsxowZlZ5OzcycOTPM805XvuuuuzKzr7/+Ohx7zz33hHm5tm3blpnlLeWtXLkyzOfPL/1tJXnLyieeeGKY520vft111+33nPbJO9W5UjjyA4mi/ECiKD+QKMoPJIryA4mi/ECiKD+QqIZa59+yZUvJY3fu3FnBmdRW3mXBr7jiijAfMmRIZrZ69epwbN5W1OV69dVXM7MzzzwzHJt3qnM5l1OP5iVJp512WsnfuxjRexjeeeedqj72Phz5gURRfiBRlB9IFOUHEkX5gURRfiBRlB9IVFlbdO/3g+Vs0Z23ttre3p6ZXXzxxeHYvMtnozoK+zp06cgjjwzH5l2LIM+AAQMysy+++CIcO2XKlDB/6aWXwry5uTnMv/zyy8xsxYry9r4pdotujvxAoig/kCjKDySK8gOJovxAoig/kCjKDyQq93x+M2uTdImkDe4+pHBbH0nPShogabWkMe7+bbmT+fjjj8M8ek/C4MGDw7Gs89dH9DMrdx0/T3Sdgzwvv/xymJezBXejKObI/5SkH2+kfqekN9x9oKQ3Cl8D6EZyy+/uCyX9+HIvl0vat83MTEnxpWYANJxSf+dvdvcOSSp8PLpyUwJQC1W/hp+ZjZc0vtqPA2D/lHrkX29mfSWp8HFD1h3dvdXdW9y9pcTHAlAFpZZ/rqRxhc/HSXqxMtMBUCu55TezOZLelfQbM1trZr+VdL+k4Wb2qaThha8BdCO5v/O7+9iM6MIKzyX3uv1r1qzJzIYOHRqOfeihh0qaE7qvgQMHljz2008/reBMGhPv8AMSRfmBRFF+IFGUH0gU5QcSRfmBRDXUFt15Hn/88cxs2rRp4dgRI0aE+WuvvRbmTU1Nmdm2bdvCsd15+/DubNCgQZlZR0dHOHbTpk2Vnk7D4cgPJIryA4mi/ECiKD+QKMoPJIryA4mi/ECiGmqL7jw9e/bMzJYuXRqO7devX5i/9dZbYX7hhdlnMD/zzDPh2LvvvjvML7nkkjBvbW0N81RFW3BL0vz58zOzdevWhWOHDRtWwowaA1t0AwhRfiBRlB9IFOUHEkX5gURRfiBRlB9IVLda54+ccMIJYf7AAw+E+ahRo8K8R48e+z2nYuX9DJYvXx7ma9euzcw2bMjcTCl3bDHjt2/fHuajR4/OzPKuc5C3Dfatt94a5tH3v+GGG8Kxc+fODfNGxjo/gBDlBxJF+YFEUX4gUZQfSBTlBxJF+YFE5a7zm1mbpEskbXD3IYXbpkr6naSvC3eb7O6v5D5YFdf588ybNy/MW1pawjxar+7fv384dvbs2WG+ePHiMM9biz/mmGMys759+4Zjjz322DDv1atXmNfTc889F+Y33XRTZpZ3Pn93Vsl1/qckjezi9ofc/eTC/3KLD6Cx5Jbf3RdK2liDuQCooXJ+559oZv80szYzO7xiMwJQE6WW/8+Sfi3pZEkdkv6YdUczG29m7WbWXuJjAaiCksrv7uvdfbe775H0uKQzgvu2unuLu8d/UQNQUyWV38w6/wl5tKQVlZkOgFrJ3aLbzOZIGibpSDNbK2mKpGFmdrIkl7Ra0u+rOEcAVXDAnM8/dOjQMM+7Lv/EiRPD/NFHH83MzjrrrHDsokWLwnzEiBFhvmDBgjCvpqampjDPew9DdJ2F4447Lhz74IMPhvmkSZPCPFWczw8gRPmBRFF+IFGUH0gU5QcSRfmBRB0wS315y2l5p64OHjw4zKNLVN94443h2MceeyzMm5ubwzzv8tn1NGvWrDA/9dRTM7OPPvooHJu3TXbeUuH3338f5gcqlvoAhCg/kCjKDySK8gOJovxAoig/kCjKDyQq93z+RnLZZZdlZnmn1V5//fVhnrfVdCTvPQTbtm0L80Zex8+zZ8+eMI+2Np8+fXo49uqrrw7za665JsxnzpwZ5qnjyA8kivIDiaL8QKIoP5Aoyg8kivIDiaL8QKIaap3fLD4Nedq0aZnZihXxviFz5swpaU7FyNvGOm+dvzvbvXt3mEfr/HnXYFiyZEmY33zzzWHOOn+MIz+QKMoPJIryA4mi/ECiKD+QKMoPJIryA4nKXec3s/6SnpZ0jKQ9klrdfbqZ9ZH0rKQBklZLGuPu35YzmUMPPbTkPG+L7bz16HLkrfNv3bq1ao9db3nn8x90UOnHl0ceeSTM8/YMOPfcczOzhQsXljSnA0kxP5ldkm519/+UdKakP5jZCZLulPSGuw+U9EbhawDdRG753b3D3ZcVPt8saaWkfpIul7TvLVQzJV1RrUkCqLz9ek1mZgMknSJpsaRmd++Q9v4DIenoSk8OQPUU/d5+M/u5pL9LusXdN+W9D7/TuPGSxpc2PQDVUtSR38x6am/xZ7v7Pwo3rzezvoW8r6Qur0Lp7q3u3uLuLZWYMIDKyC2/7T3EPyFppbv/qVM0V9K4wufjJL1Y+ekBqJZiXvafLel6ScvN7MPCbZMl3S/pb2b2W0lrJMXXWS7Cli1bwnzgwIHlPkRV5C31HchbRZdzSm+eZ599Nszvu+++MI9O+WWpr4jyu/vbkrJ+wb+wstMBUCu8ww9IFOUHEkX5gURRfiBRlB9IFOUHEtVQl+7urljnz1bOOv/OnTvDvLW1NcynTJmSmR1//PHh2FWrVoX5gYAjP5Aoyg8kivIDiaL8QKIoP5Aoyg8kivIDiWKdvwK+/Ta+YvmaNWtqNJPa++6778J8x44dVXvsGTNmhPnkyZMzswkTJoRjb7vttpLm1J1w5AcSRfmBRFF+IFGUH0gU5QcSRfmBRFF+IFHm7rV7MLPaPVgN5W1DnZfv2rWrktOpqaampjA/6qijMrPPP/+80tP5gba2tsxs9OjR4dho3lJj/8zcvai99DjyA4mi/ECiKD+QKMoPJIryA4mi/ECiKD+QqNzz+c2sv6SnJR0jaY+kVnefbmZTJf1O0teFu05291eqNdFGtmfPnrLy7mzTpk1l5dU0derUzGz58uXh2EZex6+UYi7msUvSre6+zMx+Iel9M1tQyB5y9werNz0A1ZJbfnfvkNRR+Hyzma2U1K/aEwNQXfv1O7+ZDZB0iqTFhZsmmtk/zazNzA7PGDPezNrNrL2smQKoqKLLb2Y/l/R3Sbe4+yZJf5b0a0kna+8rgz92Nc7dW929xd1bKjBfABVSVPnNrKf2Fn+2u/9Dktx9vbvvdvc9kh6XdEb1pgmg0nLLb2Ym6QlJK939T51u79vpbqMlraj89ABUS+4pvWY2VNJbkpZr71KfJE2WNFZ7X/K7pNWSfl/442D0vQ7IU3qBRlLsKb2czw8cYDifH0CI8gOJovxAoig/kCjKDySK8gOJovxAoig/kCjKDySK8gOJovxAoig/kCjKDySK8gOJKubqvZX0jaT/6fT1kYXbGlGjzq1R5yUxt1JVcm7/Uewda3o+/08e3Ky9Ua/t16hza9R5ScytVPWaGy/7gURRfiBR9S5/a50fP9Koc2vUeUnMrVR1mVtdf+cHUD/1PvIDqJO6lN/MRprZJ2b2mZndWY85ZDGz1Wa23Mw+rPcWY4Vt0DaY2YpOt/UxswVm9mnhY5fbpNVpblPN7H8Lz92HZnZxnebW38zeNLOVZvaRmd1cuL2uz10wr7o8bzV/2W9mPST9W9JwSWslLZU01t3/VdOJZDCz1ZJa3L3ua8Jmdq6k7yQ97e5DCrf9t6SN7n5/4R/Ow939vxpkblMlfVfvnZsLG8r07byztKQrJN2gOj53wbzGqA7PWz2O/GdI+szdV7n7DknPSLq8DvNoeO6+UNLGH918uaSZhc9nau9/PDWXMbeG4O4d7r6s8PlmSft2lq7rcxfMqy7qUf5+kr7s9PVaNdaW3y7pNTN738zG13syXWjetzNS4ePRdZ7Pj+Xu3FxLP9pZumGeu1J2vK60epS/q91EGmnJ4Wx3P1XSKEl/KLy8RXGK2rm5VrrYWbohlLrjdaXVo/xrJfXv9PUvJX1Vh3l0yd2/KnzcIOl5Nd7uw+v3bZJa+LihzvP5f420c3NXO0urAZ67Rtrxuh7lXyppoJn9ysx+JulaSXPrMI+fMLPehT/EyMx6Sxqhxtt9eK6kcYXPx0l6sY5z+YFG2bk5a2dp1fm5a7Qdr+vyJp/CUsbDknpIanP3e2s+iS6Y2fHae7SX9p7x+Nd6zs3M5kgapr1nfa2XNEXSC5L+Juk4SWskXe3uNf/DW8bchmk/d26u0tyydpZerDo+d5Xc8boi8+EdfkCaeIcfkCjKDySK8gOJovxAoig/kCjKDySK8gOJovxAov4P0GBNaVL7gCQAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"gan = GAN(input_dim = (28,28,1)\n        , discriminator_conv_filters = [64,64,128,128]\n        , discriminator_conv_kernel_size = [5,5,5,5]\n        , discriminator_conv_strides = [2,2,2,1]\n        , discriminator_batch_norm_momentum = None\n        , discriminator_activation = 'relu'\n        , discriminator_dropout_rate = 0.4\n        , discriminator_learning_rate = 0.0008\n        , generator_initial_dense_layer_size = (7, 7, 64)\n        , generator_upsample = [2,2, 1, 1]\n        , generator_conv_filters = [128,64, 64,1]\n        , generator_conv_kernel_size = [5,5,5,5]\n        , generator_conv_strides = [1,1, 1, 1]\n        , generator_batch_norm_momentum = 0.9\n        , generator_activation = 'relu'\n        , generator_dropout_rate = None\n        , generator_learning_rate = 0.0004\n        , optimiser = 'rmsprop'\n        , z_dim = 100\n        )\n\nif mode == 'build':\n    gan.save(RUN_FOLDER)\nelse:\n    gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))","metadata":{"execution":{"iopub.status.busy":"2024-03-19T04:11:52.361760Z","iopub.execute_input":"2024-03-19T04:11:52.362125Z","iopub.status.idle":"2024-03-19T04:11:53.618828Z","shell.execute_reply.started":"2024-03-19T04:11:52.362071Z","shell.execute_reply":"2024-03-19T04:11:53.617461Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\nEPOCHS = 6000\nPRINT_EVERY_N_BATCHES = 5","metadata":{"execution":{"iopub.status.busy":"2024-03-19T04:10:42.976152Z","iopub.execute_input":"2024-03-19T04:10:42.976551Z","iopub.status.idle":"2024-03-19T04:10:42.982126Z","shell.execute_reply.started":"2024-03-19T04:10:42.976477Z","shell.execute_reply":"2024-03-19T04:10:42.980870Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"gan.trainable=True\ngan.train(     \n    x_train\n    , batch_size = BATCH_SIZE\n    , epochs = EPOCHS\n    , run_folder = RUN_FOLDER\n    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T04:14:43.810118Z","iopub.execute_input":"2024-03-19T04:14:43.810542Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"5 [D loss: (0.720)(R 0.344, F 1.095)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.361] [G acc: 1.000]\n6 [D loss: (0.649)(R 0.390, F 0.908)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.345] [G acc: 1.000]\n7 [D loss: (0.640)(R 0.400, F 0.880)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.334] [G acc: 1.000]\n8 [D loss: (0.634)(R 0.362, F 0.906)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.336] [G acc: 1.000]\n9 [D loss: (0.663)(R 0.377, F 0.949)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.352] [G acc: 1.000]\n10 [D loss: (0.683)(R 0.389, F 0.977)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.402] [G acc: 1.000]\n11 [D loss: (0.696)(R 0.423, F 0.969)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.467] [G acc: 1.000]\n12 [D loss: (0.699)(R 0.460, F 0.938)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.523] [G acc: 1.000]\n13 [D loss: (0.703)(R 0.492, F 0.913)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.583] [G acc: 1.000]\n14 [D loss: (0.690)(R 0.513, F 0.868)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.623] [G acc: 1.000]\n15 [D loss: (0.676)(R 0.526, F 0.825)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.664] [G acc: 0.938]\n16 [D loss: (0.652)(R 0.491, F 0.813)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.662] [G acc: 0.594]\n17 [D loss: (1.124)(R 0.470, F 1.778)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.636] [G acc: 1.000]\n18 [D loss: (0.693)(R 0.594, F 0.792)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.635] [G acc: 1.000]\n19 [D loss: (0.699)(R 0.598, F 0.799)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.648] [G acc: 1.000]\n20 [D loss: (0.693)(R 0.614, F 0.772)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.653] [G acc: 1.000]\n21 [D loss: (0.683)(R 0.619, F 0.747)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.645] [G acc: 0.969]\n22 [D loss: (0.667)(R 0.616, F 0.719)] [D acc: (0.656)(1.000, 0.312)] [G loss: 0.625] [G acc: 1.000]\n23 [D loss: (0.658)(R 0.598, F 0.718)] [D acc: (0.742)(0.969, 0.516)] [G loss: 0.590] [G acc: 0.953]\n24 [D loss: (0.595)(R 0.578, F 0.613)] [D acc: (0.984)(0.984, 0.984)] [G loss: 0.419] [G acc: 1.000]\n25 [D loss: (0.485)(R 0.499, F 0.471)] [D acc: (0.992)(0.984, 1.000)] [G loss: 0.200] [G acc: 1.000]\n26 [D loss: (0.432)(R 0.485, F 0.379)] [D acc: (0.891)(0.797, 0.984)] [G loss: 0.065] [G acc: 1.000]\n27 [D loss: (0.119)(R 0.185, F 0.052)] [D acc: (0.992)(0.984, 1.000)] [G loss: 0.008] [G acc: 1.000]\n28 [D loss: (0.062)(R 0.088, F 0.036)] [D acc: (0.984)(0.984, 0.984)] [G loss: 0.002] [G acc: 1.000]\n29 [D loss: (0.070)(R 0.066, F 0.073)] [D acc: (0.977)(0.984, 0.969)] [G loss: 0.001] [G acc: 1.000]\n30 [D loss: (0.431)(R 0.168, F 0.694)] [D acc: (0.828)(0.922, 0.734)] [G loss: 0.025] [G acc: 1.000]\n31 [D loss: (0.353)(R 0.379, F 0.326)] [D acc: (0.844)(0.844, 0.844)] [G loss: 0.018] [G acc: 1.000]\n32 [D loss: (0.526)(R 0.198, F 0.854)] [D acc: (0.773)(0.938, 0.609)] [G loss: 0.063] [G acc: 1.000]\n33 [D loss: (0.972)(R 0.632, F 1.313)] [D acc: (0.516)(0.766, 0.266)] [G loss: 0.431] [G acc: 1.000]\n34 [D loss: (0.757)(R 0.577, F 0.937)] [D acc: (0.539)(0.969, 0.109)] [G loss: 0.508] [G acc: 1.000]\n35 [D loss: (0.782)(R 0.634, F 0.931)] [D acc: (0.438)(0.875, 0.000)] [G loss: 0.649] [G acc: 0.969]\n36 [D loss: (0.721)(R 0.664, F 0.778)] [D acc: (0.477)(0.953, 0.000)] [G loss: 0.693] [G acc: 0.484]\n37 [D loss: (0.699)(R 0.682, F 0.717)] [D acc: (0.438)(0.797, 0.078)] [G loss: 0.720] [G acc: 0.047]\n38 [D loss: (0.693)(R 0.692, F 0.695)] [D acc: (0.523)(0.594, 0.453)] [G loss: 0.734] [G acc: 0.000]\n39 [D loss: (0.690)(R 0.691, F 0.690)] [D acc: (0.594)(0.531, 0.656)] [G loss: 0.755] [G acc: 0.000]\n40 [D loss: (0.680)(R 0.688, F 0.672)] [D acc: (0.766)(0.609, 0.922)] [G loss: 0.799] [G acc: 0.000]\n41 [D loss: (0.664)(R 0.672, F 0.656)] [D acc: (0.859)(0.719, 1.000)] [G loss: 0.909] [G acc: 0.000]\n42 [D loss: (0.376)(R 0.620, F 0.131)] [D acc: (0.938)(0.875, 1.000)] [G loss: 1.684] [G acc: 0.031]\n43 [D loss: (0.592)(R 0.634, F 0.549)] [D acc: (0.859)(0.719, 1.000)] [G loss: 1.499] [G acc: 0.016]\n44 [D loss: (0.491)(R 0.571, F 0.410)] [D acc: (0.836)(0.688, 0.984)] [G loss: 1.972] [G acc: 0.031]\n45 [D loss: (0.224)(R 0.225, F 0.224)] [D acc: (0.969)(0.953, 0.984)] [G loss: 2.864] [G acc: 0.031]\n46 [D loss: (1.667)(R 0.256, F 3.077)] [D acc: (0.438)(0.828, 0.047)] [G loss: 3.668] [G acc: 0.000]\n47 [D loss: (0.927)(R 1.424, F 0.430)] [D acc: (0.516)(0.062, 0.969)] [G loss: 1.190] [G acc: 0.125]\n48 [D loss: (0.844)(R 0.583, F 1.104)] [D acc: (0.414)(0.688, 0.141)] [G loss: 0.743] [G acc: 0.469]\n49 [D loss: (0.383)(R 0.548, F 0.217)] [D acc: (0.930)(0.859, 1.000)] [G loss: 0.751] [G acc: 0.625]\n50 [D loss: (0.827)(R 0.448, F 1.205)] [D acc: (0.523)(0.875, 0.172)] [G loss: 0.771] [G acc: 0.484]\n51 [D loss: (0.498)(R 0.551, F 0.446)] [D acc: (0.789)(0.797, 0.781)] [G loss: 0.655] [G acc: 0.719]\n52 [D loss: (0.402)(R 0.549, F 0.254)] [D acc: (0.859)(0.828, 0.891)] [G loss: 0.535] [G acc: 0.812]\n53 [D loss: (0.353)(R 0.570, F 0.136)] [D acc: (0.867)(0.781, 0.953)] [G loss: 0.395] [G acc: 0.938]\n54 [D loss: (0.295)(R 0.432, F 0.158)] [D acc: (0.914)(0.906, 0.922)] [G loss: 0.314] [G acc: 0.969]\n55 [D loss: (0.256)(R 0.469, F 0.042)] [D acc: (0.953)(0.922, 0.984)] [G loss: 0.199] [G acc: 1.000]\n56 [D loss: (0.144)(R 0.252, F 0.036)] [D acc: (0.992)(0.984, 1.000)] [G loss: 0.103] [G acc: 0.984]\n57 [D loss: (0.085)(R 0.163, F 0.007)] [D acc: (1.000)(1.000, 1.000)] [G loss: 0.040] [G acc: 1.000]\n58 [D loss: (0.147)(R 0.077, F 0.217)] [D acc: (0.953)(1.000, 0.906)] [G loss: 0.102] [G acc: 0.969]\n59 [D loss: (2.090)(R 0.353, F 3.827)] [D acc: (0.414)(0.812, 0.016)] [G loss: 0.787] [G acc: 0.250]\n60 [D loss: (0.791)(R 0.657, F 0.925)] [D acc: (0.320)(0.625, 0.016)] [G loss: 0.665] [G acc: 0.625]\n61 [D loss: (0.728)(R 0.603, F 0.854)] [D acc: (0.461)(0.906, 0.016)] [G loss: 0.656] [G acc: 0.688]\n62 [D loss: (0.722)(R 0.626, F 0.817)] [D acc: (0.477)(0.906, 0.047)] [G loss: 0.661] [G acc: 0.703]\n63 [D loss: (0.699)(R 0.621, F 0.777)] [D acc: (0.500)(0.922, 0.078)] [G loss: 0.684] [G acc: 0.438]\n64 [D loss: (0.700)(R 0.623, F 0.777)] [D acc: (0.523)(0.938, 0.109)] [G loss: 0.708] [G acc: 0.375]\n65 [D loss: (0.696)(R 0.628, F 0.764)] [D acc: (0.508)(0.891, 0.125)] [G loss: 0.726] [G acc: 0.312]\n66 [D loss: (0.710)(R 0.646, F 0.774)] [D acc: (0.523)(0.875, 0.172)] [G loss: 0.717] [G acc: 0.250]\n67 [D loss: (0.673)(R 0.630, F 0.715)] [D acc: (0.609)(0.906, 0.312)] [G loss: 0.736] [G acc: 0.219]\n68 [D loss: (0.665)(R 0.628, F 0.702)] [D acc: (0.664)(0.891, 0.438)] [G loss: 0.786] [G acc: 0.109]\n69 [D loss: (0.631)(R 0.584, F 0.678)] [D acc: (0.734)(0.938, 0.531)] [G loss: 0.923] [G acc: 0.062]\n70 [D loss: (0.683)(R 0.640, F 0.725)] [D acc: (0.562)(0.750, 0.375)] [G loss: 0.825] [G acc: 0.047]\n71 [D loss: (0.652)(R 0.573, F 0.731)] [D acc: (0.656)(0.875, 0.438)] [G loss: 0.904] [G acc: 0.016]\n72 [D loss: (0.629)(R 0.552, F 0.706)] [D acc: (0.664)(0.859, 0.469)] [G loss: 1.009] [G acc: 0.000]\n73 [D loss: (0.726)(R 0.640, F 0.812)] [D acc: (0.492)(0.688, 0.297)] [G loss: 0.795] [G acc: 0.078]\n74 [D loss: (0.624)(R 0.565, F 0.683)] [D acc: (0.703)(0.906, 0.500)] [G loss: 1.214] [G acc: 0.000]\n75 [D loss: (0.647)(R 0.610, F 0.685)] [D acc: (0.680)(0.750, 0.609)] [G loss: 1.490] [G acc: 0.000]\n76 [D loss: (0.601)(R 0.488, F 0.714)] [D acc: (0.711)(0.953, 0.469)] [G loss: 1.703] [G acc: 0.000]\n77 [D loss: (0.600)(R 0.708, F 0.493)] [D acc: (0.766)(0.641, 0.891)] [G loss: 1.320] [G acc: 0.031]\n78 [D loss: (0.665)(R 0.596, F 0.734)] [D acc: (0.656)(0.781, 0.531)] [G loss: 1.359] [G acc: 0.000]\n79 [D loss: (0.590)(R 0.477, F 0.704)] [D acc: (0.695)(0.891, 0.500)] [G loss: 1.250] [G acc: 0.000]\n80 [D loss: (0.663)(R 0.516, F 0.809)] [D acc: (0.633)(0.875, 0.391)] [G loss: 1.029] [G acc: 0.016]\n81 [D loss: (0.456)(R 0.422, F 0.489)] [D acc: (0.891)(0.984, 0.797)] [G loss: 2.393] [G acc: 0.000]\n82 [D loss: (0.635)(R 0.677, F 0.594)] [D acc: (0.703)(0.703, 0.703)] [G loss: 2.462] [G acc: 0.000]\n83 [D loss: (0.600)(R 0.340, F 0.861)] [D acc: (0.656)(0.938, 0.375)] [G loss: 1.992] [G acc: 0.000]\n84 [D loss: (0.380)(R 0.521, F 0.238)] [D acc: (0.844)(0.750, 0.938)] [G loss: 1.053] [G acc: 0.156]\n85 [D loss: (0.375)(R 0.211, F 0.538)] [D acc: (0.820)(0.953, 0.688)] [G loss: 1.508] [G acc: 0.062]\n86 [D loss: (0.579)(R 0.345, F 0.813)] [D acc: (0.664)(0.844, 0.484)] [G loss: 2.503] [G acc: 0.000]\n87 [D loss: (0.682)(R 0.717, F 0.647)] [D acc: (0.609)(0.578, 0.641)] [G loss: 1.277] [G acc: 0.000]\n88 [D loss: (0.501)(R 0.474, F 0.527)] [D acc: (0.805)(0.875, 0.734)] [G loss: 1.404] [G acc: 0.031]\n89 [D loss: (0.774)(R 0.522, F 1.026)] [D acc: (0.461)(0.766, 0.156)] [G loss: 1.560] [G acc: 0.000]\n90 [D loss: (0.573)(R 0.659, F 0.488)] [D acc: (0.711)(0.500, 0.922)] [G loss: 1.105] [G acc: 0.031]\n91 [D loss: (0.605)(R 0.396, F 0.813)] [D acc: (0.664)(0.891, 0.438)] [G loss: 1.306] [G acc: 0.016]\n92 [D loss: (0.633)(R 0.554, F 0.713)] [D acc: (0.648)(0.672, 0.625)] [G loss: 1.140] [G acc: 0.000]\n93 [D loss: (0.648)(R 0.514, F 0.782)] [D acc: (0.617)(0.797, 0.438)] [G loss: 0.984] [G acc: 0.047]\n94 [D loss: (0.664)(R 0.651, F 0.677)] [D acc: (0.633)(0.641, 0.625)] [G loss: 0.903] [G acc: 0.125]\n95 [D loss: (0.681)(R 0.510, F 0.852)] [D acc: (0.547)(0.797, 0.297)] [G loss: 1.009] [G acc: 0.000]\n96 [D loss: (0.622)(R 0.626, F 0.618)] [D acc: (0.750)(0.703, 0.797)] [G loss: 0.987] [G acc: 0.016]\n97 [D loss: (0.637)(R 0.520, F 0.754)] [D acc: (0.617)(0.766, 0.469)] [G loss: 1.020] [G acc: 0.016]\n98 [D loss: (0.616)(R 0.589, F 0.644)] [D acc: (0.695)(0.734, 0.656)] [G loss: 1.074] [G acc: 0.016]\n99 [D loss: (0.604)(R 0.499, F 0.709)] [D acc: (0.664)(0.781, 0.547)] [G loss: 1.086] [G acc: 0.016]\n100 [D loss: (0.526)(R 0.423, F 0.630)] [D acc: (0.758)(0.875, 0.641)] [G loss: 1.433] [G acc: 0.000]\n101 [D loss: (0.590)(R 0.506, F 0.674)] [D acc: (0.695)(0.781, 0.609)] [G loss: 1.702] [G acc: 0.000]\n102 [D loss: (0.415)(R 0.401, F 0.428)] [D acc: (0.914)(0.891, 0.938)] [G loss: 2.638] [G acc: 0.000]\n103 [D loss: (0.573)(R 0.214, F 0.931)] [D acc: (0.641)(0.953, 0.328)] [G loss: 2.218] [G acc: 0.000]\n104 [D loss: (0.675)(R 0.882, F 0.468)] [D acc: (0.695)(0.469, 0.922)] [G loss: 1.526] [G acc: 0.047]\n105 [D loss: (0.456)(R 0.283, F 0.629)] [D acc: (0.797)(0.922, 0.672)] [G loss: 1.896] [G acc: 0.000]\n106 [D loss: (0.501)(R 0.452, F 0.551)] [D acc: (0.797)(0.750, 0.844)] [G loss: 1.521] [G acc: 0.000]\n107 [D loss: (0.400)(R 0.316, F 0.484)] [D acc: (0.883)(0.906, 0.859)] [G loss: 1.679] [G acc: 0.000]\n108 [D loss: (0.334)(R 0.226, F 0.441)] [D acc: (0.883)(0.922, 0.844)] [G loss: 2.208] [G acc: 0.000]\n109 [D loss: (0.826)(R 0.420, F 1.232)] [D acc: (0.586)(0.859, 0.312)] [G loss: 1.806] [G acc: 0.000]\n110 [D loss: (0.627)(R 0.703, F 0.551)] [D acc: (0.742)(0.578, 0.906)] [G loss: 1.432] [G acc: 0.000]\n111 [D loss: (0.511)(R 0.352, F 0.669)] [D acc: (0.758)(0.891, 0.625)] [G loss: 1.789] [G acc: 0.000]\n112 [D loss: (0.531)(R 0.516, F 0.545)] [D acc: (0.773)(0.750, 0.797)] [G loss: 1.468] [G acc: 0.016]\n113 [D loss: (0.802)(R 0.255, F 1.348)] [D acc: (0.609)(0.953, 0.266)] [G loss: 1.493] [G acc: 0.000]\n114 [D loss: (0.698)(R 0.818, F 0.577)] [D acc: (0.609)(0.438, 0.781)] [G loss: 1.064] [G acc: 0.062]\n115 [D loss: (0.637)(R 0.468, F 0.805)] [D acc: (0.711)(0.891, 0.531)] [G loss: 0.943] [G acc: 0.078]\n116 [D loss: (0.622)(R 0.457, F 0.787)] [D acc: (0.703)(0.844, 0.562)] [G loss: 0.888] [G acc: 0.156]\n117 [D loss: (0.654)(R 0.501, F 0.808)] [D acc: (0.633)(0.719, 0.547)] [G loss: 0.899] [G acc: 0.172]\n118 [D loss: (0.709)(R 0.579, F 0.839)] [D acc: (0.539)(0.672, 0.406)] [G loss: 0.903] [G acc: 0.141]\n119 [D loss: (0.700)(R 0.572, F 0.829)] [D acc: (0.547)(0.672, 0.422)] [G loss: 0.844] [G acc: 0.219]\n120 [D loss: (0.687)(R 0.576, F 0.798)] [D acc: (0.594)(0.750, 0.438)] [G loss: 0.806] [G acc: 0.234]\n121 [D loss: (0.712)(R 0.607, F 0.817)] [D acc: (0.547)(0.672, 0.422)] [G loss: 0.744] [G acc: 0.312]\n122 [D loss: (0.684)(R 0.569, F 0.800)] [D acc: (0.570)(0.812, 0.328)] [G loss: 0.744] [G acc: 0.469]\n123 [D loss: (0.705)(R 0.565, F 0.844)] [D acc: (0.516)(0.812, 0.219)] [G loss: 0.729] [G acc: 0.438]\n124 [D loss: (0.697)(R 0.627, F 0.766)] [D acc: (0.523)(0.688, 0.359)] [G loss: 0.738] [G acc: 0.375]\n125 [D loss: (0.673)(R 0.615, F 0.731)] [D acc: (0.531)(0.641, 0.422)] [G loss: 0.695] [G acc: 0.422]\n126 [D loss: (0.735)(R 0.612, F 0.858)] [D acc: (0.430)(0.672, 0.188)] [G loss: 0.709] [G acc: 0.422]\n127 [D loss: (0.684)(R 0.642, F 0.726)] [D acc: (0.531)(0.734, 0.328)] [G loss: 0.668] [G acc: 0.703]\n128 [D loss: (0.673)(R 0.603, F 0.742)] [D acc: (0.562)(0.781, 0.344)] [G loss: 0.701] [G acc: 0.406]\n129 [D loss: (0.681)(R 0.592, F 0.771)] [D acc: (0.500)(0.734, 0.266)] [G loss: 0.687] [G acc: 0.531]\n130 [D loss: (0.716)(R 0.666, F 0.767)] [D acc: (0.445)(0.625, 0.266)] [G loss: 0.689] [G acc: 0.578]\n131 [D loss: (0.692)(R 0.652, F 0.732)] [D acc: (0.539)(0.750, 0.328)] [G loss: 0.682] [G acc: 0.594]\n132 [D loss: (0.686)(R 0.644, F 0.728)] [D acc: (0.492)(0.734, 0.250)] [G loss: 0.695] [G acc: 0.406]\n133 [D loss: (0.696)(R 0.650, F 0.742)] [D acc: (0.484)(0.688, 0.281)] [G loss: 0.704] [G acc: 0.422]\n134 [D loss: (0.682)(R 0.658, F 0.705)] [D acc: (0.539)(0.688, 0.391)] [G loss: 0.709] [G acc: 0.391]\n135 [D loss: (0.692)(R 0.643, F 0.740)] [D acc: (0.500)(0.734, 0.266)] [G loss: 0.712] [G acc: 0.359]\n136 [D loss: (0.684)(R 0.654, F 0.714)] [D acc: (0.531)(0.656, 0.406)] [G loss: 0.714] [G acc: 0.266]\n137 [D loss: (0.722)(R 0.604, F 0.840)] [D acc: (0.562)(0.844, 0.281)] [G loss: 0.725] [G acc: 0.188]\n138 [D loss: (0.680)(R 0.665, F 0.694)] [D acc: (0.555)(0.594, 0.516)] [G loss: 0.727] [G acc: 0.266]\n139 [D loss: (0.663)(R 0.604, F 0.722)] [D acc: (0.617)(0.781, 0.453)] [G loss: 0.712] [G acc: 0.375]\n140 [D loss: (0.693)(R 0.598, F 0.789)] [D acc: (0.531)(0.766, 0.297)] [G loss: 0.713] [G acc: 0.328]\n141 [D loss: (0.666)(R 0.646, F 0.685)] [D acc: (0.680)(0.734, 0.625)] [G loss: 0.710] [G acc: 0.406]\n142 [D loss: (0.697)(R 0.603, F 0.791)] [D acc: (0.500)(0.719, 0.281)] [G loss: 0.703] [G acc: 0.406]\n143 [D loss: (0.679)(R 0.625, F 0.733)] [D acc: (0.617)(0.781, 0.453)] [G loss: 0.706] [G acc: 0.297]\n144 [D loss: (0.672)(R 0.592, F 0.752)] [D acc: (0.547)(0.688, 0.406)] [G loss: 0.713] [G acc: 0.297]\n145 [D loss: (0.682)(R 0.608, F 0.757)] [D acc: (0.539)(0.766, 0.312)] [G loss: 0.708] [G acc: 0.312]\n146 [D loss: (0.689)(R 0.621, F 0.757)] [D acc: (0.500)(0.719, 0.281)] [G loss: 0.699] [G acc: 0.391]\n147 [D loss: (0.681)(R 0.629, F 0.733)] [D acc: (0.555)(0.750, 0.359)] [G loss: 0.692] [G acc: 0.406]\n148 [D loss: (0.688)(R 0.628, F 0.748)] [D acc: (0.586)(0.734, 0.438)] [G loss: 0.721] [G acc: 0.250]\n149 [D loss: (0.679)(R 0.604, F 0.755)] [D acc: (0.531)(0.781, 0.281)] [G loss: 0.707] [G acc: 0.344]\n150 [D loss: (0.675)(R 0.622, F 0.729)] [D acc: (0.578)(0.766, 0.391)] [G loss: 0.700] [G acc: 0.312]\n151 [D loss: (0.704)(R 0.578, F 0.829)] [D acc: (0.516)(0.766, 0.266)] [G loss: 0.712] [G acc: 0.297]\n152 [D loss: (0.666)(R 0.642, F 0.690)] [D acc: (0.633)(0.719, 0.547)] [G loss: 0.729] [G acc: 0.219]\n153 [D loss: (0.695)(R 0.603, F 0.787)] [D acc: (0.469)(0.734, 0.203)] [G loss: 0.703] [G acc: 0.312]\n154 [D loss: (0.687)(R 0.631, F 0.744)] [D acc: (0.617)(0.828, 0.406)] [G loss: 0.708] [G acc: 0.266]\n155 [D loss: (0.686)(R 0.646, F 0.725)] [D acc: (0.523)(0.703, 0.344)] [G loss: 0.696] [G acc: 0.359]\n156 [D loss: (0.697)(R 0.621, F 0.774)] [D acc: (0.516)(0.766, 0.266)] [G loss: 0.706] [G acc: 0.234]\n157 [D loss: (0.680)(R 0.637, F 0.722)] [D acc: (0.594)(0.797, 0.391)] [G loss: 0.700] [G acc: 0.328]\n158 [D loss: (0.663)(R 0.586, F 0.740)] [D acc: (0.602)(0.797, 0.406)] [G loss: 0.716] [G acc: 0.156]\n159 [D loss: (0.687)(R 0.640, F 0.734)] [D acc: (0.578)(0.703, 0.453)] [G loss: 0.702] [G acc: 0.250]\n160 [D loss: (0.701)(R 0.653, F 0.749)] [D acc: (0.578)(0.797, 0.359)] [G loss: 0.716] [G acc: 0.172]\n161 [D loss: (0.682)(R 0.652, F 0.712)] [D acc: (0.516)(0.656, 0.375)] [G loss: 0.711] [G acc: 0.172]\n162 [D loss: (0.673)(R 0.624, F 0.722)] [D acc: (0.602)(0.688, 0.516)] [G loss: 0.723] [G acc: 0.188]\n163 [D loss: (0.686)(R 0.632, F 0.740)] [D acc: (0.461)(0.594, 0.328)] [G loss: 0.698] [G acc: 0.297]\n164 [D loss: (0.698)(R 0.686, F 0.710)] [D acc: (0.383)(0.594, 0.172)] [G loss: 0.700] [G acc: 0.297]\n165 [D loss: (0.690)(R 0.677, F 0.703)] [D acc: (0.508)(0.594, 0.422)] [G loss: 0.707] [G acc: 0.219]\n166 [D loss: (0.684)(R 0.622, F 0.747)] [D acc: (0.547)(0.719, 0.375)] [G loss: 0.701] [G acc: 0.297]\n167 [D loss: (0.670)(R 0.599, F 0.741)] [D acc: (0.664)(0.797, 0.531)] [G loss: 0.727] [G acc: 0.203]\n168 [D loss: (0.668)(R 0.597, F 0.740)] [D acc: (0.578)(0.719, 0.438)] [G loss: 0.721] [G acc: 0.125]\n169 [D loss: (0.678)(R 0.616, F 0.739)] [D acc: (0.555)(0.609, 0.500)] [G loss: 0.727] [G acc: 0.047]\n170 [D loss: (0.702)(R 0.582, F 0.823)] [D acc: (0.523)(0.797, 0.250)] [G loss: 0.731] [G acc: 0.109]\n171 [D loss: (0.654)(R 0.601, F 0.706)] [D acc: (0.578)(0.594, 0.562)] [G loss: 0.730] [G acc: 0.078]\n172 [D loss: (0.633)(R 0.531, F 0.735)] [D acc: (0.594)(0.781, 0.406)] [G loss: 0.720] [G acc: 0.188]\n173 [D loss: (0.666)(R 0.504, F 0.828)] [D acc: (0.594)(0.812, 0.375)] [G loss: 0.730] [G acc: 0.094]\n174 [D loss: (0.684)(R 0.620, F 0.749)] [D acc: (0.547)(0.703, 0.391)] [G loss: 0.734] [G acc: 0.141]\n175 [D loss: (0.636)(R 0.509, F 0.763)] [D acc: (0.617)(0.766, 0.469)] [G loss: 0.754] [G acc: 0.047]\n176 [D loss: (0.736)(R 0.447, F 1.026)] [D acc: (0.531)(0.812, 0.250)] [G loss: 0.771] [G acc: 0.125]\n177 [D loss: (0.626)(R 0.605, F 0.646)] [D acc: (0.750)(0.625, 0.875)] [G loss: 0.928] [G acc: 0.000]\n178 [D loss: (0.664)(R 0.461, F 0.866)] [D acc: (0.594)(0.859, 0.328)] [G loss: 0.745] [G acc: 0.281]\n179 [D loss: (0.715)(R 0.736, F 0.694)] [D acc: (0.594)(0.594, 0.594)] [G loss: 0.930] [G acc: 0.000]\n180 [D loss: (0.714)(R 0.653, F 0.776)] [D acc: (0.438)(0.516, 0.359)] [G loss: 0.723] [G acc: 0.406]\n181 [D loss: (0.634)(R 0.583, F 0.684)] [D acc: (0.742)(0.812, 0.672)] [G loss: 0.799] [G acc: 0.062]\n182 [D loss: (0.775)(R 0.562, F 0.989)] [D acc: (0.422)(0.781, 0.062)] [G loss: 0.675] [G acc: 0.562]\n183 [D loss: (0.704)(R 0.611, F 0.797)] [D acc: (0.469)(0.781, 0.156)] [G loss: 0.667] [G acc: 0.547]\n184 [D loss: (0.691)(R 0.585, F 0.798)] [D acc: (0.508)(0.859, 0.156)] [G loss: 0.681] [G acc: 0.562]\n185 [D loss: (0.721)(R 0.586, F 0.856)] [D acc: (0.492)(0.828, 0.156)] [G loss: 0.700] [G acc: 0.406]\n186 [D loss: (0.689)(R 0.618, F 0.760)] [D acc: (0.547)(0.844, 0.250)] [G loss: 0.721] [G acc: 0.297]\n187 [D loss: (0.694)(R 0.634, F 0.754)] [D acc: (0.492)(0.734, 0.250)] [G loss: 0.719] [G acc: 0.266]\n188 [D loss: (0.688)(R 0.642, F 0.733)] [D acc: (0.547)(0.719, 0.375)] [G loss: 0.714] [G acc: 0.219]\n189 [D loss: (0.686)(R 0.660, F 0.712)] [D acc: (0.586)(0.703, 0.469)] [G loss: 0.714] [G acc: 0.297]\n190 [D loss: (0.672)(R 0.634, F 0.709)] [D acc: (0.633)(0.781, 0.484)] [G loss: 0.727] [G acc: 0.297]\n191 [D loss: (0.688)(R 0.665, F 0.712)] [D acc: (0.523)(0.609, 0.438)] [G loss: 0.883] [G acc: 0.078]\n192 [D loss: (0.650)(R 0.647, F 0.654)] [D acc: (0.688)(0.719, 0.656)] [G loss: 0.942] [G acc: 0.047]\n193 [D loss: (0.694)(R 0.579, F 0.809)] [D acc: (0.539)(0.797, 0.281)] [G loss: 0.738] [G acc: 0.281]\n194 [D loss: (0.684)(R 0.605, F 0.763)] [D acc: (0.523)(0.750, 0.297)] [G loss: 0.715] [G acc: 0.312]\n195 [D loss: (0.676)(R 0.617, F 0.736)] [D acc: (0.578)(0.781, 0.375)] [G loss: 0.712] [G acc: 0.344]\n196 [D loss: (0.689)(R 0.626, F 0.751)] [D acc: (0.492)(0.719, 0.266)] [G loss: 0.711] [G acc: 0.344]\n197 [D loss: (0.700)(R 0.632, F 0.768)] [D acc: (0.492)(0.719, 0.266)] [G loss: 0.712] [G acc: 0.297]\n198 [D loss: (0.689)(R 0.635, F 0.743)] [D acc: (0.516)(0.734, 0.297)] [G loss: 0.731] [G acc: 0.219]\n199 [D loss: (0.690)(R 0.621, F 0.759)] [D acc: (0.523)(0.750, 0.297)] [G loss: 0.704] [G acc: 0.359]\n200 [D loss: (0.682)(R 0.614, F 0.750)] [D acc: (0.570)(0.828, 0.312)] [G loss: 0.701] [G acc: 0.312]\n201 [D loss: (0.689)(R 0.625, F 0.753)] [D acc: (0.570)(0.750, 0.391)] [G loss: 0.714] [G acc: 0.359]\n202 [D loss: (0.686)(R 0.635, F 0.737)] [D acc: (0.531)(0.750, 0.312)] [G loss: 0.720] [G acc: 0.219]\n203 [D loss: (0.684)(R 0.636, F 0.732)] [D acc: (0.555)(0.734, 0.375)] [G loss: 0.707] [G acc: 0.281]\n204 [D loss: (0.674)(R 0.629, F 0.720)] [D acc: (0.602)(0.828, 0.375)] [G loss: 0.709] [G acc: 0.344]\n205 [D loss: (0.704)(R 0.642, F 0.766)] [D acc: (0.477)(0.719, 0.234)] [G loss: 0.725] [G acc: 0.219]\n206 [D loss: (0.674)(R 0.655, F 0.694)] [D acc: (0.680)(0.766, 0.594)] [G loss: 0.726] [G acc: 0.312]\n207 [D loss: (0.697)(R 0.661, F 0.734)] [D acc: (0.516)(0.656, 0.375)] [G loss: 0.734] [G acc: 0.172]\n208 [D loss: (0.666)(R 0.646, F 0.685)] [D acc: (0.672)(0.672, 0.672)] [G loss: 0.715] [G acc: 0.344]\n209 [D loss: (0.699)(R 0.633, F 0.765)] [D acc: (0.531)(0.766, 0.297)] [G loss: 0.729] [G acc: 0.266]\n210 [D loss: (0.680)(R 0.651, F 0.708)] [D acc: (0.547)(0.672, 0.422)] [G loss: 0.728] [G acc: 0.250]\n211 [D loss: (0.658)(R 0.629, F 0.686)] [D acc: (0.672)(0.719, 0.625)] [G loss: 0.822] [G acc: 0.031]\n212 [D loss: (0.679)(R 0.623, F 0.735)] [D acc: (0.602)(0.703, 0.500)] [G loss: 0.869] [G acc: 0.078]\n213 [D loss: (0.724)(R 0.645, F 0.803)] [D acc: (0.477)(0.625, 0.328)] [G loss: 0.845] [G acc: 0.031]\n214 [D loss: (0.693)(R 0.673, F 0.713)] [D acc: (0.484)(0.531, 0.438)] [G loss: 0.746] [G acc: 0.172]\n215 [D loss: (0.687)(R 0.618, F 0.756)] [D acc: (0.602)(0.844, 0.359)] [G loss: 0.716] [G acc: 0.234]\n216 [D loss: (0.686)(R 0.637, F 0.734)] [D acc: (0.508)(0.750, 0.266)] [G loss: 0.719] [G acc: 0.266]\n217 [D loss: (0.682)(R 0.637, F 0.727)] [D acc: (0.578)(0.750, 0.406)] [G loss: 0.717] [G acc: 0.250]\n218 [D loss: (0.685)(R 0.632, F 0.738)] [D acc: (0.586)(0.812, 0.359)] [G loss: 0.729] [G acc: 0.203]\n219 [D loss: (0.691)(R 0.636, F 0.747)] [D acc: (0.508)(0.734, 0.281)] [G loss: 0.721] [G acc: 0.219]\n220 [D loss: (0.684)(R 0.663, F 0.704)] [D acc: (0.547)(0.656, 0.438)] [G loss: 0.731] [G acc: 0.203]\n221 [D loss: (0.670)(R 0.624, F 0.715)] [D acc: (0.562)(0.734, 0.391)] [G loss: 0.722] [G acc: 0.266]\n222 [D loss: (0.678)(R 0.630, F 0.726)] [D acc: (0.594)(0.734, 0.453)] [G loss: 0.750] [G acc: 0.125]\n223 [D loss: (0.667)(R 0.625, F 0.708)] [D acc: (0.508)(0.656, 0.359)] [G loss: 0.739] [G acc: 0.125]\n224 [D loss: (0.683)(R 0.659, F 0.708)] [D acc: (0.555)(0.594, 0.516)] [G loss: 0.724] [G acc: 0.234]\n225 [D loss: (0.669)(R 0.612, F 0.726)] [D acc: (0.547)(0.750, 0.344)] [G loss: 0.737] [G acc: 0.078]\n226 [D loss: (0.691)(R 0.631, F 0.752)] [D acc: (0.539)(0.688, 0.391)] [G loss: 0.749] [G acc: 0.172]\n227 [D loss: (0.688)(R 0.646, F 0.731)] [D acc: (0.586)(0.641, 0.531)] [G loss: 0.725] [G acc: 0.141]\n228 [D loss: (0.686)(R 0.642, F 0.729)] [D acc: (0.633)(0.781, 0.484)] [G loss: 0.737] [G acc: 0.172]\n229 [D loss: (0.692)(R 0.617, F 0.767)] [D acc: (0.508)(0.734, 0.281)] [G loss: 0.742] [G acc: 0.219]\n230 [D loss: (0.667)(R 0.637, F 0.697)] [D acc: (0.648)(0.688, 0.609)] [G loss: 0.747] [G acc: 0.141]\n231 [D loss: (0.691)(R 0.639, F 0.743)] [D acc: (0.570)(0.750, 0.391)] [G loss: 0.755] [G acc: 0.094]\n232 [D loss: (0.667)(R 0.617, F 0.718)] [D acc: (0.656)(0.781, 0.531)] [G loss: 0.736] [G acc: 0.172]\n233 [D loss: (0.674)(R 0.596, F 0.751)] [D acc: (0.648)(0.844, 0.453)] [G loss: 0.731] [G acc: 0.219]\n234 [D loss: (0.677)(R 0.602, F 0.753)] [D acc: (0.570)(0.766, 0.375)] [G loss: 0.742] [G acc: 0.125]\n235 [D loss: (0.671)(R 0.607, F 0.735)] [D acc: (0.609)(0.781, 0.438)] [G loss: 0.748] [G acc: 0.172]\n236 [D loss: (0.663)(R 0.629, F 0.697)] [D acc: (0.562)(0.656, 0.469)] [G loss: 0.756] [G acc: 0.156]\n237 [D loss: (0.686)(R 0.552, F 0.820)] [D acc: (0.539)(0.797, 0.281)] [G loss: 0.823] [G acc: 0.141]\n238 [D loss: (0.674)(R 0.659, F 0.688)] [D acc: (0.609)(0.609, 0.609)] [G loss: 0.767] [G acc: 0.125]\n239 [D loss: (0.652)(R 0.585, F 0.719)] [D acc: (0.672)(0.828, 0.516)] [G loss: 0.808] [G acc: 0.203]\n240 [D loss: (0.664)(R 0.597, F 0.732)] [D acc: (0.531)(0.641, 0.422)] [G loss: 0.864] [G acc: 0.000]\n241 [D loss: (0.692)(R 0.626, F 0.759)] [D acc: (0.461)(0.578, 0.344)] [G loss: 0.776] [G acc: 0.141]\n242 [D loss: (0.691)(R 0.602, F 0.780)] [D acc: (0.531)(0.688, 0.375)] [G loss: 0.747] [G acc: 0.172]\n243 [D loss: (0.690)(R 0.614, F 0.767)] [D acc: (0.602)(0.797, 0.406)] [G loss: 0.741] [G acc: 0.234]\n244 [D loss: (0.670)(R 0.606, F 0.733)] [D acc: (0.609)(0.766, 0.453)] [G loss: 0.732] [G acc: 0.312]\n245 [D loss: (0.695)(R 0.618, F 0.772)] [D acc: (0.586)(0.766, 0.406)] [G loss: 0.721] [G acc: 0.375]\n246 [D loss: (0.640)(R 0.572, F 0.708)] [D acc: (0.641)(0.766, 0.516)] [G loss: 0.754] [G acc: 0.297]\n247 [D loss: (0.680)(R 0.601, F 0.759)] [D acc: (0.547)(0.703, 0.391)] [G loss: 0.872] [G acc: 0.047]\n248 [D loss: (0.655)(R 0.613, F 0.698)] [D acc: (0.633)(0.703, 0.562)] [G loss: 0.822] [G acc: 0.109]\n249 [D loss: (0.672)(R 0.584, F 0.760)] [D acc: (0.602)(0.812, 0.391)] [G loss: 0.768] [G acc: 0.156]\n250 [D loss: (0.646)(R 0.534, F 0.757)] [D acc: (0.617)(0.828, 0.406)] [G loss: 0.777] [G acc: 0.203]\n251 [D loss: (0.700)(R 0.617, F 0.782)] [D acc: (0.539)(0.688, 0.391)] [G loss: 0.761] [G acc: 0.281]\n252 [D loss: (0.681)(R 0.618, F 0.743)] [D acc: (0.555)(0.703, 0.406)] [G loss: 0.784] [G acc: 0.172]\n253 [D loss: (0.665)(R 0.588, F 0.742)] [D acc: (0.609)(0.766, 0.453)] [G loss: 0.755] [G acc: 0.219]\n254 [D loss: (0.652)(R 0.576, F 0.728)] [D acc: (0.617)(0.750, 0.484)] [G loss: 0.764] [G acc: 0.219]\n255 [D loss: (0.671)(R 0.604, F 0.739)] [D acc: (0.555)(0.734, 0.375)] [G loss: 0.747] [G acc: 0.250]\n256 [D loss: (0.661)(R 0.611, F 0.711)] [D acc: (0.555)(0.625, 0.484)] [G loss: 0.769] [G acc: 0.203]\n257 [D loss: (0.672)(R 0.630, F 0.713)] [D acc: (0.609)(0.703, 0.516)] [G loss: 0.837] [G acc: 0.109]\n258 [D loss: (0.668)(R 0.658, F 0.678)] [D acc: (0.594)(0.578, 0.609)] [G loss: 0.814] [G acc: 0.078]\n259 [D loss: (0.685)(R 0.623, F 0.747)] [D acc: (0.609)(0.688, 0.531)] [G loss: 0.783] [G acc: 0.078]\n260 [D loss: (0.651)(R 0.597, F 0.705)] [D acc: (0.617)(0.672, 0.562)] [G loss: 0.823] [G acc: 0.141]\n261 [D loss: (0.652)(R 0.591, F 0.714)] [D acc: (0.633)(0.672, 0.594)] [G loss: 0.836] [G acc: 0.062]\n262 [D loss: (0.669)(R 0.569, F 0.769)] [D acc: (0.570)(0.734, 0.406)] [G loss: 0.783] [G acc: 0.250]\n263 [D loss: (0.673)(R 0.568, F 0.778)] [D acc: (0.508)(0.688, 0.328)] [G loss: 0.770] [G acc: 0.141]\n264 [D loss: (0.669)(R 0.609, F 0.730)] [D acc: (0.602)(0.656, 0.547)] [G loss: 0.767] [G acc: 0.203]\n265 [D loss: (0.674)(R 0.578, F 0.770)] [D acc: (0.586)(0.797, 0.375)] [G loss: 0.758] [G acc: 0.234]\n266 [D loss: (0.662)(R 0.609, F 0.715)] [D acc: (0.617)(0.719, 0.516)] [G loss: 0.762] [G acc: 0.297]\n267 [D loss: (0.673)(R 0.540, F 0.806)] [D acc: (0.594)(0.844, 0.344)] [G loss: 0.771] [G acc: 0.172]\n268 [D loss: (0.660)(R 0.614, F 0.706)] [D acc: (0.641)(0.688, 0.594)] [G loss: 0.780] [G acc: 0.250]\n269 [D loss: (0.633)(R 0.557, F 0.710)] [D acc: (0.648)(0.734, 0.562)] [G loss: 0.790] [G acc: 0.297]\n270 [D loss: (0.648)(R 0.640, F 0.656)] [D acc: (0.680)(0.594, 0.766)] [G loss: 0.875] [G acc: 0.172]\n271 [D loss: (0.654)(R 0.528, F 0.781)] [D acc: (0.578)(0.766, 0.391)] [G loss: 0.863] [G acc: 0.156]\n272 [D loss: (0.621)(R 0.556, F 0.686)] [D acc: (0.711)(0.781, 0.641)] [G loss: 0.853] [G acc: 0.281]\n273 [D loss: (0.697)(R 0.597, F 0.798)] [D acc: (0.578)(0.734, 0.422)] [G loss: 0.856] [G acc: 0.094]\n274 [D loss: (0.689)(R 0.606, F 0.773)] [D acc: (0.539)(0.688, 0.391)] [G loss: 0.756] [G acc: 0.234]\n275 [D loss: (0.631)(R 0.561, F 0.701)] [D acc: (0.680)(0.859, 0.500)] [G loss: 0.802] [G acc: 0.203]\n276 [D loss: (0.706)(R 0.554, F 0.859)] [D acc: (0.531)(0.766, 0.297)] [G loss: 0.810] [G acc: 0.172]\n277 [D loss: (0.669)(R 0.593, F 0.745)] [D acc: (0.609)(0.750, 0.469)] [G loss: 0.783] [G acc: 0.203]\n278 [D loss: (0.673)(R 0.588, F 0.757)] [D acc: (0.617)(0.750, 0.484)] [G loss: 0.758] [G acc: 0.281]\n279 [D loss: (0.690)(R 0.630, F 0.751)] [D acc: (0.570)(0.641, 0.500)] [G loss: 0.800] [G acc: 0.188]\n280 [D loss: (0.668)(R 0.655, F 0.681)] [D acc: (0.578)(0.547, 0.609)] [G loss: 0.766] [G acc: 0.266]\n281 [D loss: (0.712)(R 0.651, F 0.774)] [D acc: (0.555)(0.609, 0.500)] [G loss: 0.789] [G acc: 0.266]\n282 [D loss: (0.692)(R 0.639, F 0.744)] [D acc: (0.531)(0.641, 0.422)] [G loss: 0.745] [G acc: 0.250]\n283 [D loss: (0.669)(R 0.633, F 0.706)] [D acc: (0.555)(0.641, 0.469)] [G loss: 0.749] [G acc: 0.203]\n284 [D loss: (0.667)(R 0.593, F 0.742)] [D acc: (0.578)(0.703, 0.453)] [G loss: 0.731] [G acc: 0.328]\n285 [D loss: (0.667)(R 0.561, F 0.774)] [D acc: (0.641)(0.797, 0.484)] [G loss: 0.757] [G acc: 0.203]\n286 [D loss: (0.658)(R 0.601, F 0.715)] [D acc: (0.648)(0.719, 0.578)] [G loss: 0.788] [G acc: 0.219]\n287 [D loss: (0.637)(R 0.561, F 0.714)] [D acc: (0.633)(0.766, 0.500)] [G loss: 0.784] [G acc: 0.188]\n288 [D loss: (0.660)(R 0.577, F 0.743)] [D acc: (0.617)(0.750, 0.484)] [G loss: 0.787] [G acc: 0.219]\n289 [D loss: (0.670)(R 0.623, F 0.717)] [D acc: (0.617)(0.625, 0.609)] [G loss: 0.821] [G acc: 0.141]\n290 [D loss: (0.663)(R 0.626, F 0.701)] [D acc: (0.578)(0.672, 0.484)] [G loss: 0.833] [G acc: 0.156]\n291 [D loss: (0.669)(R 0.598, F 0.741)] [D acc: (0.602)(0.703, 0.500)] [G loss: 0.822] [G acc: 0.062]\n292 [D loss: (0.675)(R 0.561, F 0.789)] [D acc: (0.539)(0.609, 0.469)] [G loss: 0.787] [G acc: 0.141]\n293 [D loss: (0.679)(R 0.624, F 0.735)] [D acc: (0.539)(0.641, 0.438)] [G loss: 0.813] [G acc: 0.141]\n294 [D loss: (0.661)(R 0.609, F 0.713)] [D acc: (0.562)(0.688, 0.438)] [G loss: 0.769] [G acc: 0.219]\n295 [D loss: (0.657)(R 0.594, F 0.720)] [D acc: (0.641)(0.703, 0.578)] [G loss: 0.793] [G acc: 0.094]\n296 [D loss: (0.638)(R 0.570, F 0.706)] [D acc: (0.664)(0.766, 0.562)] [G loss: 0.821] [G acc: 0.156]\n297 [D loss: (0.665)(R 0.608, F 0.722)] [D acc: (0.586)(0.641, 0.531)] [G loss: 0.779] [G acc: 0.172]\n298 [D loss: (0.668)(R 0.587, F 0.749)] [D acc: (0.570)(0.672, 0.469)] [G loss: 0.796] [G acc: 0.109]\n299 [D loss: (0.649)(R 0.592, F 0.706)] [D acc: (0.633)(0.703, 0.562)] [G loss: 0.808] [G acc: 0.141]\n300 [D loss: (0.633)(R 0.578, F 0.687)] [D acc: (0.586)(0.609, 0.562)] [G loss: 0.822] [G acc: 0.141]\n301 [D loss: (0.659)(R 0.615, F 0.703)] [D acc: (0.602)(0.641, 0.562)] [G loss: 0.792] [G acc: 0.250]\n302 [D loss: (0.647)(R 0.581, F 0.713)] [D acc: (0.594)(0.672, 0.516)] [G loss: 0.821] [G acc: 0.094]\n303 [D loss: (0.608)(R 0.595, F 0.622)] [D acc: (0.734)(0.750, 0.719)] [G loss: 1.168] [G acc: 0.047]\n304 [D loss: (0.633)(R 0.642, F 0.624)] [D acc: (0.633)(0.578, 0.688)] [G loss: 1.012] [G acc: 0.203]\n305 [D loss: (0.704)(R 0.535, F 0.874)] [D acc: (0.609)(0.812, 0.406)] [G loss: 0.826] [G acc: 0.156]\n306 [D loss: (0.612)(R 0.528, F 0.697)] [D acc: (0.750)(0.812, 0.688)] [G loss: 0.828] [G acc: 0.172]\n307 [D loss: (0.680)(R 0.572, F 0.787)] [D acc: (0.562)(0.688, 0.438)] [G loss: 0.847] [G acc: 0.172]\n308 [D loss: (0.646)(R 0.578, F 0.715)] [D acc: (0.625)(0.703, 0.547)] [G loss: 0.826] [G acc: 0.203]\n309 [D loss: (0.663)(R 0.589, F 0.737)] [D acc: (0.617)(0.750, 0.484)] [G loss: 0.856] [G acc: 0.203]\n310 [D loss: (0.672)(R 0.616, F 0.727)] [D acc: (0.539)(0.641, 0.438)] [G loss: 0.827] [G acc: 0.172]\n311 [D loss: (0.652)(R 0.628, F 0.676)] [D acc: (0.672)(0.641, 0.703)] [G loss: 0.825] [G acc: 0.188]\n312 [D loss: (0.666)(R 0.584, F 0.749)] [D acc: (0.602)(0.703, 0.500)] [G loss: 0.795] [G acc: 0.266]\n313 [D loss: (0.634)(R 0.598, F 0.670)] [D acc: (0.664)(0.656, 0.672)] [G loss: 0.866] [G acc: 0.156]\n314 [D loss: (0.648)(R 0.603, F 0.692)] [D acc: (0.625)(0.656, 0.594)] [G loss: 0.850] [G acc: 0.094]\n315 [D loss: (0.589)(R 0.518, F 0.660)] [D acc: (0.711)(0.812, 0.609)] [G loss: 0.856] [G acc: 0.172]\n316 [D loss: (0.613)(R 0.535, F 0.692)] [D acc: (0.703)(0.781, 0.625)] [G loss: 0.854] [G acc: 0.328]\n317 [D loss: (0.669)(R 0.556, F 0.782)] [D acc: (0.578)(0.672, 0.484)] [G loss: 0.955] [G acc: 0.109]\n318 [D loss: (0.658)(R 0.665, F 0.650)] [D acc: (0.578)(0.516, 0.641)] [G loss: 0.998] [G acc: 0.094]\n319 [D loss: (0.661)(R 0.608, F 0.715)] [D acc: (0.641)(0.625, 0.656)] [G loss: 0.920] [G acc: 0.078]\n320 [D loss: (0.623)(R 0.548, F 0.697)] [D acc: (0.617)(0.734, 0.500)] [G loss: 0.861] [G acc: 0.188]\n321 [D loss: (0.676)(R 0.613, F 0.739)] [D acc: (0.555)(0.609, 0.500)] [G loss: 0.869] [G acc: 0.141]\n322 [D loss: (0.615)(R 0.557, F 0.674)] [D acc: (0.633)(0.656, 0.609)] [G loss: 0.824] [G acc: 0.266]\n323 [D loss: (0.666)(R 0.541, F 0.791)] [D acc: (0.578)(0.688, 0.469)] [G loss: 0.835] [G acc: 0.219]\n324 [D loss: (0.650)(R 0.604, F 0.696)] [D acc: (0.656)(0.672, 0.641)] [G loss: 0.835] [G acc: 0.203]\n325 [D loss: (0.661)(R 0.602, F 0.720)] [D acc: (0.570)(0.641, 0.500)] [G loss: 0.857] [G acc: 0.172]\n326 [D loss: (0.674)(R 0.605, F 0.743)] [D acc: (0.602)(0.672, 0.531)] [G loss: 0.849] [G acc: 0.156]\n327 [D loss: (0.637)(R 0.590, F 0.684)] [D acc: (0.625)(0.688, 0.562)] [G loss: 0.851] [G acc: 0.188]\n328 [D loss: (0.657)(R 0.598, F 0.716)] [D acc: (0.609)(0.625, 0.594)] [G loss: 0.905] [G acc: 0.172]\n329 [D loss: (0.679)(R 0.624, F 0.735)] [D acc: (0.570)(0.609, 0.531)] [G loss: 0.877] [G acc: 0.078]\n330 [D loss: (0.632)(R 0.613, F 0.652)] [D acc: (0.711)(0.672, 0.750)] [G loss: 0.863] [G acc: 0.156]\n331 [D loss: (0.663)(R 0.595, F 0.731)] [D acc: (0.617)(0.672, 0.562)] [G loss: 0.878] [G acc: 0.125]\n332 [D loss: (0.650)(R 0.596, F 0.704)] [D acc: (0.594)(0.656, 0.531)] [G loss: 0.915] [G acc: 0.219]\n333 [D loss: (0.609)(R 0.595, F 0.623)] [D acc: (0.727)(0.656, 0.797)] [G loss: 0.957] [G acc: 0.141]\n334 [D loss: (0.621)(R 0.584, F 0.658)] [D acc: (0.680)(0.719, 0.641)] [G loss: 1.010] [G acc: 0.109]\n335 [D loss: (0.633)(R 0.583, F 0.683)] [D acc: (0.633)(0.656, 0.609)] [G loss: 0.881] [G acc: 0.234]\n336 [D loss: (0.680)(R 0.558, F 0.801)] [D acc: (0.594)(0.656, 0.531)] [G loss: 0.909] [G acc: 0.172]\n337 [D loss: (0.635)(R 0.585, F 0.684)] [D acc: (0.664)(0.703, 0.625)] [G loss: 0.899] [G acc: 0.188]\n338 [D loss: (0.671)(R 0.627, F 0.714)] [D acc: (0.609)(0.688, 0.531)] [G loss: 0.893] [G acc: 0.125]\n339 [D loss: (0.626)(R 0.559, F 0.693)] [D acc: (0.648)(0.688, 0.609)] [G loss: 0.907] [G acc: 0.156]\n340 [D loss: (0.644)(R 0.598, F 0.689)] [D acc: (0.648)(0.703, 0.594)] [G loss: 0.877] [G acc: 0.172]\n341 [D loss: (0.650)(R 0.629, F 0.672)] [D acc: (0.609)(0.562, 0.656)] [G loss: 0.934] [G acc: 0.094]\n342 [D loss: (0.659)(R 0.633, F 0.686)] [D acc: (0.602)(0.609, 0.594)] [G loss: 0.872] [G acc: 0.156]\n343 [D loss: (0.612)(R 0.557, F 0.667)] [D acc: (0.703)(0.719, 0.688)] [G loss: 0.871] [G acc: 0.188]\n344 [D loss: (0.680)(R 0.600, F 0.761)] [D acc: (0.594)(0.609, 0.578)] [G loss: 0.859] [G acc: 0.172]\n345 [D loss: (0.617)(R 0.550, F 0.685)] [D acc: (0.680)(0.781, 0.578)] [G loss: 0.860] [G acc: 0.203]\n346 [D loss: (0.621)(R 0.568, F 0.674)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.013] [G acc: 0.109]\n347 [D loss: (0.632)(R 0.604, F 0.660)] [D acc: (0.680)(0.625, 0.734)] [G loss: 0.976] [G acc: 0.109]\n348 [D loss: (0.676)(R 0.550, F 0.801)] [D acc: (0.578)(0.688, 0.469)] [G loss: 0.916] [G acc: 0.094]\n349 [D loss: (0.575)(R 0.497, F 0.652)] [D acc: (0.789)(0.797, 0.781)] [G loss: 0.974] [G acc: 0.125]\n350 [D loss: (0.627)(R 0.577, F 0.678)] [D acc: (0.648)(0.656, 0.641)] [G loss: 1.003] [G acc: 0.094]\n351 [D loss: (0.651)(R 0.572, F 0.731)] [D acc: (0.586)(0.719, 0.453)] [G loss: 0.956] [G acc: 0.125]\n352 [D loss: (0.662)(R 0.591, F 0.733)] [D acc: (0.602)(0.672, 0.531)] [G loss: 0.950] [G acc: 0.219]\n353 [D loss: (0.597)(R 0.554, F 0.641)] [D acc: (0.688)(0.688, 0.688)] [G loss: 0.980] [G acc: 0.141]\n354 [D loss: (0.653)(R 0.582, F 0.724)] [D acc: (0.641)(0.672, 0.609)] [G loss: 0.983] [G acc: 0.125]\n355 [D loss: (0.644)(R 0.612, F 0.677)] [D acc: (0.625)(0.641, 0.609)] [G loss: 0.862] [G acc: 0.328]\n356 [D loss: (0.616)(R 0.508, F 0.724)] [D acc: (0.703)(0.750, 0.656)] [G loss: 0.897] [G acc: 0.219]\n357 [D loss: (0.561)(R 0.484, F 0.638)] [D acc: (0.750)(0.797, 0.703)] [G loss: 0.898] [G acc: 0.188]\n358 [D loss: (0.660)(R 0.511, F 0.809)] [D acc: (0.594)(0.703, 0.484)] [G loss: 1.029] [G acc: 0.141]\n359 [D loss: (0.582)(R 0.523, F 0.641)] [D acc: (0.680)(0.656, 0.703)] [G loss: 0.905] [G acc: 0.188]\n360 [D loss: (0.601)(R 0.572, F 0.631)] [D acc: (0.703)(0.734, 0.672)] [G loss: 0.964] [G acc: 0.125]\n361 [D loss: (0.692)(R 0.585, F 0.799)] [D acc: (0.555)(0.641, 0.469)] [G loss: 1.010] [G acc: 0.109]\n362 [D loss: (0.629)(R 0.655, F 0.603)] [D acc: (0.594)(0.484, 0.703)] [G loss: 1.023] [G acc: 0.078]\n363 [D loss: (0.602)(R 0.580, F 0.624)] [D acc: (0.734)(0.734, 0.734)] [G loss: 1.004] [G acc: 0.109]\n364 [D loss: (0.644)(R 0.522, F 0.767)] [D acc: (0.633)(0.781, 0.484)] [G loss: 1.005] [G acc: 0.156]\n365 [D loss: (0.672)(R 0.613, F 0.732)] [D acc: (0.641)(0.688, 0.594)] [G loss: 0.933] [G acc: 0.141]\n366 [D loss: (0.634)(R 0.647, F 0.621)] [D acc: (0.648)(0.531, 0.766)] [G loss: 0.930] [G acc: 0.172]\n367 [D loss: (0.639)(R 0.662, F 0.616)] [D acc: (0.648)(0.516, 0.781)] [G loss: 0.950] [G acc: 0.172]\n368 [D loss: (0.632)(R 0.560, F 0.704)] [D acc: (0.633)(0.703, 0.562)] [G loss: 0.966] [G acc: 0.172]\n369 [D loss: (0.609)(R 0.538, F 0.680)] [D acc: (0.648)(0.672, 0.625)] [G loss: 1.009] [G acc: 0.203]\n370 [D loss: (0.705)(R 0.640, F 0.769)] [D acc: (0.539)(0.516, 0.562)] [G loss: 1.002] [G acc: 0.125]\n371 [D loss: (0.667)(R 0.669, F 0.664)] [D acc: (0.562)(0.484, 0.641)] [G loss: 0.958] [G acc: 0.062]\n372 [D loss: (0.625)(R 0.630, F 0.620)] [D acc: (0.688)(0.625, 0.750)] [G loss: 0.995] [G acc: 0.078]\n373 [D loss: (0.681)(R 0.616, F 0.746)] [D acc: (0.570)(0.578, 0.562)] [G loss: 0.974] [G acc: 0.031]\n374 [D loss: (0.647)(R 0.610, F 0.685)] [D acc: (0.602)(0.578, 0.625)] [G loss: 0.903] [G acc: 0.172]\n375 [D loss: (0.617)(R 0.615, F 0.620)] [D acc: (0.711)(0.641, 0.781)] [G loss: 0.919] [G acc: 0.125]\n376 [D loss: (0.641)(R 0.580, F 0.703)] [D acc: (0.609)(0.641, 0.578)] [G loss: 0.911] [G acc: 0.078]\n377 [D loss: (0.603)(R 0.551, F 0.655)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.001] [G acc: 0.109]\n378 [D loss: (0.643)(R 0.580, F 0.707)] [D acc: (0.641)(0.594, 0.688)] [G loss: 0.983] [G acc: 0.062]\n379 [D loss: (0.638)(R 0.540, F 0.736)] [D acc: (0.648)(0.703, 0.594)] [G loss: 1.000] [G acc: 0.094]\n380 [D loss: (0.629)(R 0.518, F 0.739)] [D acc: (0.672)(0.750, 0.594)] [G loss: 0.959] [G acc: 0.109]\n381 [D loss: (0.624)(R 0.621, F 0.626)] [D acc: (0.617)(0.516, 0.719)] [G loss: 0.980] [G acc: 0.125]\n382 [D loss: (0.643)(R 0.688, F 0.598)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.002] [G acc: 0.047]\n383 [D loss: (0.556)(R 0.489, F 0.622)] [D acc: (0.766)(0.781, 0.750)] [G loss: 0.931] [G acc: 0.156]\n384 [D loss: (0.641)(R 0.551, F 0.732)] [D acc: (0.641)(0.703, 0.578)] [G loss: 0.934] [G acc: 0.141]\n385 [D loss: (0.642)(R 0.461, F 0.823)] [D acc: (0.633)(0.828, 0.438)] [G loss: 0.979] [G acc: 0.109]\n386 [D loss: (0.583)(R 0.584, F 0.583)] [D acc: (0.672)(0.641, 0.703)] [G loss: 0.931] [G acc: 0.188]\n387 [D loss: (0.603)(R 0.547, F 0.660)] [D acc: (0.688)(0.719, 0.656)] [G loss: 1.076] [G acc: 0.047]\n388 [D loss: (0.571)(R 0.555, F 0.586)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.001] [G acc: 0.156]\n389 [D loss: (0.608)(R 0.489, F 0.728)] [D acc: (0.656)(0.766, 0.547)] [G loss: 0.984] [G acc: 0.156]\n390 [D loss: (0.670)(R 0.678, F 0.662)] [D acc: (0.609)(0.578, 0.641)] [G loss: 0.947] [G acc: 0.078]\n391 [D loss: (0.602)(R 0.555, F 0.649)] [D acc: (0.703)(0.688, 0.719)] [G loss: 0.994] [G acc: 0.078]\n392 [D loss: (0.535)(R 0.512, F 0.559)] [D acc: (0.789)(0.750, 0.828)] [G loss: 1.081] [G acc: 0.125]\n393 [D loss: (0.636)(R 0.461, F 0.811)] [D acc: (0.609)(0.750, 0.469)] [G loss: 1.020] [G acc: 0.156]\n394 [D loss: (0.627)(R 0.553, F 0.700)] [D acc: (0.625)(0.656, 0.594)] [G loss: 1.037] [G acc: 0.141]\n395 [D loss: (0.581)(R 0.493, F 0.669)] [D acc: (0.688)(0.719, 0.656)] [G loss: 1.010] [G acc: 0.109]\n396 [D loss: (0.658)(R 0.543, F 0.772)] [D acc: (0.680)(0.734, 0.625)] [G loss: 1.337] [G acc: 0.047]\n397 [D loss: (0.671)(R 0.722, F 0.619)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.112] [G acc: 0.062]\n398 [D loss: (0.581)(R 0.517, F 0.645)] [D acc: (0.750)(0.766, 0.734)] [G loss: 1.004] [G acc: 0.141]\n399 [D loss: (0.561)(R 0.500, F 0.623)] [D acc: (0.719)(0.797, 0.641)] [G loss: 0.993] [G acc: 0.203]\n400 [D loss: (0.632)(R 0.505, F 0.760)] [D acc: (0.672)(0.750, 0.594)] [G loss: 1.081] [G acc: 0.172]\n401 [D loss: (0.653)(R 0.592, F 0.714)] [D acc: (0.648)(0.656, 0.641)] [G loss: 0.976] [G acc: 0.141]\n402 [D loss: (0.618)(R 0.548, F 0.688)] [D acc: (0.727)(0.750, 0.703)] [G loss: 0.994] [G acc: 0.188]\n403 [D loss: (0.643)(R 0.597, F 0.689)] [D acc: (0.625)(0.625, 0.625)] [G loss: 1.068] [G acc: 0.094]\n404 [D loss: (0.664)(R 0.600, F 0.727)] [D acc: (0.609)(0.672, 0.547)] [G loss: 0.939] [G acc: 0.219]\n405 [D loss: (0.623)(R 0.613, F 0.633)] [D acc: (0.641)(0.578, 0.703)] [G loss: 1.023] [G acc: 0.094]\n406 [D loss: (0.614)(R 0.589, F 0.639)] [D acc: (0.688)(0.672, 0.703)] [G loss: 0.936] [G acc: 0.234]\n407 [D loss: (0.622)(R 0.641, F 0.604)] [D acc: (0.602)(0.516, 0.688)] [G loss: 0.982] [G acc: 0.125]\n408 [D loss: (0.608)(R 0.541, F 0.675)] [D acc: (0.695)(0.719, 0.672)] [G loss: 1.117] [G acc: 0.094]\n409 [D loss: (0.677)(R 0.642, F 0.711)] [D acc: (0.633)(0.594, 0.672)] [G loss: 1.004] [G acc: 0.156]\n410 [D loss: (0.629)(R 0.633, F 0.625)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.004] [G acc: 0.141]\n411 [D loss: (0.598)(R 0.573, F 0.624)] [D acc: (0.688)(0.672, 0.703)] [G loss: 0.958] [G acc: 0.141]\n412 [D loss: (0.649)(R 0.606, F 0.693)] [D acc: (0.648)(0.625, 0.672)] [G loss: 1.034] [G acc: 0.062]\n413 [D loss: (0.609)(R 0.578, F 0.639)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.081] [G acc: 0.031]\n414 [D loss: (0.599)(R 0.615, F 0.582)] [D acc: (0.695)(0.609, 0.781)] [G loss: 0.976] [G acc: 0.219]\n415 [D loss: (0.615)(R 0.596, F 0.635)] [D acc: (0.664)(0.672, 0.656)] [G loss: 1.132] [G acc: 0.094]\n416 [D loss: (0.579)(R 0.601, F 0.557)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.128] [G acc: 0.109]\n417 [D loss: (0.672)(R 0.491, F 0.854)] [D acc: (0.633)(0.734, 0.531)] [G loss: 0.944] [G acc: 0.219]\n418 [D loss: (0.624)(R 0.550, F 0.698)] [D acc: (0.664)(0.703, 0.625)] [G loss: 0.969] [G acc: 0.141]\n419 [D loss: (0.703)(R 0.683, F 0.723)] [D acc: (0.570)(0.547, 0.594)] [G loss: 0.932] [G acc: 0.250]\n420 [D loss: (0.644)(R 0.589, F 0.698)] [D acc: (0.656)(0.656, 0.656)] [G loss: 0.916] [G acc: 0.172]\n421 [D loss: (0.630)(R 0.603, F 0.657)] [D acc: (0.656)(0.688, 0.625)] [G loss: 1.013] [G acc: 0.125]\n422 [D loss: (0.582)(R 0.550, F 0.614)] [D acc: (0.703)(0.750, 0.656)] [G loss: 1.020] [G acc: 0.172]\n423 [D loss: (0.647)(R 0.571, F 0.723)] [D acc: (0.641)(0.672, 0.609)] [G loss: 0.902] [G acc: 0.203]\n424 [D loss: (0.642)(R 0.606, F 0.678)] [D acc: (0.688)(0.672, 0.703)] [G loss: 0.977] [G acc: 0.156]\n425 [D loss: (0.640)(R 0.621, F 0.659)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.023] [G acc: 0.156]\n426 [D loss: (0.644)(R 0.550, F 0.739)] [D acc: (0.656)(0.719, 0.594)] [G loss: 1.048] [G acc: 0.078]\n427 [D loss: (0.618)(R 0.602, F 0.635)] [D acc: (0.672)(0.688, 0.656)] [G loss: 0.966] [G acc: 0.125]\n428 [D loss: (0.661)(R 0.646, F 0.677)] [D acc: (0.625)(0.609, 0.641)] [G loss: 0.978] [G acc: 0.141]\n429 [D loss: (0.623)(R 0.595, F 0.651)] [D acc: (0.672)(0.672, 0.672)] [G loss: 0.966] [G acc: 0.188]\n430 [D loss: (0.665)(R 0.651, F 0.679)] [D acc: (0.594)(0.578, 0.609)] [G loss: 0.988] [G acc: 0.141]\n431 [D loss: (0.603)(R 0.583, F 0.623)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.040] [G acc: 0.141]\n432 [D loss: (0.632)(R 0.600, F 0.664)] [D acc: (0.680)(0.719, 0.641)] [G loss: 0.983] [G acc: 0.109]\n433 [D loss: (0.637)(R 0.616, F 0.658)] [D acc: (0.617)(0.625, 0.609)] [G loss: 1.061] [G acc: 0.062]\n434 [D loss: (0.584)(R 0.578, F 0.591)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.067] [G acc: 0.172]\n435 [D loss: (0.606)(R 0.538, F 0.673)] [D acc: (0.617)(0.672, 0.562)] [G loss: 1.006] [G acc: 0.219]\n436 [D loss: (0.625)(R 0.560, F 0.690)] [D acc: (0.688)(0.734, 0.641)] [G loss: 1.072] [G acc: 0.188]\n437 [D loss: (0.659)(R 0.604, F 0.714)] [D acc: (0.609)(0.609, 0.609)] [G loss: 1.084] [G acc: 0.062]\n438 [D loss: (0.647)(R 0.680, F 0.613)] [D acc: (0.602)(0.547, 0.656)] [G loss: 1.016] [G acc: 0.125]\n439 [D loss: (0.572)(R 0.577, F 0.568)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.047] [G acc: 0.078]\n440 [D loss: (0.634)(R 0.569, F 0.698)] [D acc: (0.664)(0.703, 0.625)] [G loss: 1.069] [G acc: 0.094]\n441 [D loss: (0.638)(R 0.660, F 0.616)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.019] [G acc: 0.172]\n442 [D loss: (0.678)(R 0.639, F 0.718)] [D acc: (0.586)(0.609, 0.562)] [G loss: 1.023] [G acc: 0.125]\n443 [D loss: (0.602)(R 0.612, F 0.592)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.003] [G acc: 0.125]\n444 [D loss: (0.626)(R 0.529, F 0.723)] [D acc: (0.664)(0.734, 0.594)] [G loss: 0.957] [G acc: 0.203]\n445 [D loss: (0.661)(R 0.609, F 0.714)] [D acc: (0.594)(0.625, 0.562)] [G loss: 0.981] [G acc: 0.094]\n446 [D loss: (0.619)(R 0.616, F 0.621)] [D acc: (0.656)(0.594, 0.719)] [G loss: 0.996] [G acc: 0.188]\n447 [D loss: (0.646)(R 0.592, F 0.699)] [D acc: (0.695)(0.734, 0.656)] [G loss: 0.965] [G acc: 0.109]\n448 [D loss: (0.650)(R 0.655, F 0.645)] [D acc: (0.617)(0.547, 0.688)] [G loss: 0.927] [G acc: 0.125]\n449 [D loss: (0.630)(R 0.596, F 0.664)] [D acc: (0.594)(0.609, 0.578)] [G loss: 1.018] [G acc: 0.156]\n450 [D loss: (0.704)(R 0.610, F 0.798)] [D acc: (0.562)(0.641, 0.484)] [G loss: 0.983] [G acc: 0.109]\n451 [D loss: (0.674)(R 0.670, F 0.679)] [D acc: (0.625)(0.578, 0.672)] [G loss: 0.916] [G acc: 0.141]\n452 [D loss: (0.648)(R 0.638, F 0.658)] [D acc: (0.656)(0.641, 0.672)] [G loss: 0.887] [G acc: 0.172]\n453 [D loss: (0.602)(R 0.583, F 0.622)] [D acc: (0.641)(0.594, 0.688)] [G loss: 0.973] [G acc: 0.094]\n454 [D loss: (0.650)(R 0.624, F 0.677)] [D acc: (0.578)(0.594, 0.562)] [G loss: 0.963] [G acc: 0.094]\n455 [D loss: (0.640)(R 0.671, F 0.608)] [D acc: (0.602)(0.516, 0.688)] [G loss: 0.945] [G acc: 0.094]\n456 [D loss: (0.599)(R 0.533, F 0.665)] [D acc: (0.680)(0.766, 0.594)] [G loss: 0.950] [G acc: 0.172]\n457 [D loss: (0.620)(R 0.589, F 0.651)] [D acc: (0.648)(0.609, 0.688)] [G loss: 0.976] [G acc: 0.156]\n458 [D loss: (0.572)(R 0.518, F 0.627)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.038] [G acc: 0.062]\n459 [D loss: (0.622)(R 0.532, F 0.711)] [D acc: (0.672)(0.734, 0.609)] [G loss: 1.086] [G acc: 0.125]\n460 [D loss: (0.713)(R 0.734, F 0.692)] [D acc: (0.570)(0.516, 0.625)] [G loss: 0.934] [G acc: 0.141]\n461 [D loss: (0.557)(R 0.513, F 0.600)] [D acc: (0.750)(0.781, 0.719)] [G loss: 0.954] [G acc: 0.188]\n462 [D loss: (0.575)(R 0.560, F 0.590)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.011] [G acc: 0.156]\n463 [D loss: (0.617)(R 0.568, F 0.665)] [D acc: (0.664)(0.656, 0.672)] [G loss: 0.988] [G acc: 0.141]\n464 [D loss: (0.627)(R 0.528, F 0.725)] [D acc: (0.656)(0.688, 0.625)] [G loss: 0.917] [G acc: 0.219]\n465 [D loss: (0.631)(R 0.624, F 0.638)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.050] [G acc: 0.125]\n466 [D loss: (0.617)(R 0.529, F 0.704)] [D acc: (0.680)(0.719, 0.641)] [G loss: 1.023] [G acc: 0.141]\n467 [D loss: (0.645)(R 0.534, F 0.756)] [D acc: (0.609)(0.656, 0.562)] [G loss: 0.989] [G acc: 0.141]\n468 [D loss: (0.662)(R 0.696, F 0.628)] [D acc: (0.594)(0.516, 0.672)] [G loss: 0.955] [G acc: 0.156]\n469 [D loss: (0.576)(R 0.530, F 0.622)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.033] [G acc: 0.109]\n470 [D loss: (0.627)(R 0.594, F 0.659)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.006] [G acc: 0.141]\n471 [D loss: (0.604)(R 0.633, F 0.575)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.032] [G acc: 0.188]\n472 [D loss: (0.576)(R 0.423, F 0.730)] [D acc: (0.719)(0.828, 0.609)] [G loss: 1.106] [G acc: 0.062]\n473 [D loss: (0.672)(R 0.673, F 0.671)] [D acc: (0.617)(0.562, 0.672)] [G loss: 1.030] [G acc: 0.125]\n474 [D loss: (0.636)(R 0.612, F 0.660)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.042] [G acc: 0.141]\n475 [D loss: (0.581)(R 0.498, F 0.665)] [D acc: (0.734)(0.766, 0.703)] [G loss: 1.000] [G acc: 0.219]\n476 [D loss: (0.633)(R 0.597, F 0.668)] [D acc: (0.656)(0.672, 0.641)] [G loss: 1.083] [G acc: 0.078]\n477 [D loss: (0.623)(R 0.614, F 0.631)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.053] [G acc: 0.094]\n478 [D loss: (0.674)(R 0.629, F 0.720)] [D acc: (0.602)(0.609, 0.594)] [G loss: 1.014] [G acc: 0.141]\n479 [D loss: (0.611)(R 0.573, F 0.649)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.059] [G acc: 0.156]\n480 [D loss: (0.636)(R 0.616, F 0.655)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.050] [G acc: 0.094]\n481 [D loss: (0.626)(R 0.583, F 0.669)] [D acc: (0.641)(0.641, 0.641)] [G loss: 1.052] [G acc: 0.141]\n482 [D loss: (0.599)(R 0.560, F 0.639)] [D acc: (0.648)(0.641, 0.656)] [G loss: 1.054] [G acc: 0.125]\n483 [D loss: (0.657)(R 0.658, F 0.655)] [D acc: (0.586)(0.531, 0.641)] [G loss: 0.961] [G acc: 0.156]\n484 [D loss: (0.626)(R 0.600, F 0.653)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.039] [G acc: 0.125]\n485 [D loss: (0.585)(R 0.562, F 0.609)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.017] [G acc: 0.156]\n486 [D loss: (0.606)(R 0.608, F 0.605)] [D acc: (0.617)(0.547, 0.688)] [G loss: 0.963] [G acc: 0.203]\n487 [D loss: (0.581)(R 0.456, F 0.706)] [D acc: (0.711)(0.828, 0.594)] [G loss: 1.186] [G acc: 0.047]\n488 [D loss: (0.728)(R 0.719, F 0.737)] [D acc: (0.594)(0.531, 0.656)] [G loss: 1.042] [G acc: 0.125]\n489 [D loss: (0.661)(R 0.593, F 0.729)] [D acc: (0.570)(0.547, 0.594)] [G loss: 0.983] [G acc: 0.156]\n490 [D loss: (0.614)(R 0.614, F 0.614)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.075] [G acc: 0.094]\n491 [D loss: (0.662)(R 0.659, F 0.666)] [D acc: (0.641)(0.578, 0.703)] [G loss: 1.060] [G acc: 0.094]\n492 [D loss: (0.688)(R 0.674, F 0.702)] [D acc: (0.562)(0.547, 0.578)] [G loss: 1.032] [G acc: 0.047]\n493 [D loss: (0.634)(R 0.669, F 0.599)] [D acc: (0.625)(0.516, 0.734)] [G loss: 1.002] [G acc: 0.156]\n494 [D loss: (0.593)(R 0.625, F 0.560)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.050] [G acc: 0.125]\n495 [D loss: (0.608)(R 0.597, F 0.619)] [D acc: (0.648)(0.609, 0.688)] [G loss: 1.064] [G acc: 0.078]\n496 [D loss: (0.586)(R 0.597, F 0.576)] [D acc: (0.703)(0.609, 0.797)] [G loss: 0.995] [G acc: 0.125]\n497 [D loss: (0.601)(R 0.546, F 0.657)] [D acc: (0.664)(0.703, 0.625)] [G loss: 1.015] [G acc: 0.172]\n498 [D loss: (0.637)(R 0.635, F 0.640)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.149] [G acc: 0.109]\n499 [D loss: (0.612)(R 0.601, F 0.623)] [D acc: (0.648)(0.641, 0.656)] [G loss: 1.035] [G acc: 0.188]\n500 [D loss: (0.642)(R 0.610, F 0.674)] [D acc: (0.656)(0.625, 0.688)] [G loss: 0.975] [G acc: 0.156]\n501 [D loss: (0.663)(R 0.630, F 0.697)] [D acc: (0.562)(0.594, 0.531)] [G loss: 0.979] [G acc: 0.203]\n502 [D loss: (0.594)(R 0.608, F 0.580)] [D acc: (0.688)(0.641, 0.734)] [G loss: 0.990] [G acc: 0.156]\n503 [D loss: (0.593)(R 0.589, F 0.597)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.103] [G acc: 0.125]\n504 [D loss: (0.623)(R 0.614, F 0.632)] [D acc: (0.633)(0.656, 0.609)] [G loss: 1.004] [G acc: 0.250]\n505 [D loss: (0.587)(R 0.565, F 0.608)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.109] [G acc: 0.141]\n506 [D loss: (0.541)(R 0.531, F 0.552)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.126] [G acc: 0.109]\n507 [D loss: (0.702)(R 0.699, F 0.705)] [D acc: (0.562)(0.500, 0.625)] [G loss: 1.056] [G acc: 0.078]\n508 [D loss: (0.624)(R 0.626, F 0.623)] [D acc: (0.641)(0.609, 0.672)] [G loss: 1.002] [G acc: 0.172]\n509 [D loss: (0.627)(R 0.574, F 0.680)] [D acc: (0.633)(0.672, 0.594)] [G loss: 0.893] [G acc: 0.281]\n510 [D loss: (0.598)(R 0.476, F 0.720)] [D acc: (0.672)(0.766, 0.578)] [G loss: 0.917] [G acc: 0.266]\n511 [D loss: (0.601)(R 0.600, F 0.601)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.035] [G acc: 0.109]\n","output_type":"stream"}]},{"cell_type":"code","source":"fig = plt.figure()\nplt.plot([x[0] for x in gan.d_losses], color='black', linewidth=0.25)\n\nplt.plot([x[1] for x in gan.d_losses], color='green', linewidth=0.25)\nplt.plot([x[2] for x in gan.d_losses], color='red', linewidth=0.25)\nplt.plot([x[0] for x in gan.g_losses], color='orange', linewidth=0.25)\n\nplt.xlabel('batch', fontsize=18)\nplt.ylabel('loss', fontsize=16)\n\nplt.xlim(0, 2000)\nplt.ylim(0, 2)\n\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]}]}